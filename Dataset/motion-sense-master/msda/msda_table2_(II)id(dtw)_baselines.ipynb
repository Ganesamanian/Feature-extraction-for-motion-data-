{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py35/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.plotting import autocorrelation_plot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "######################################################################################################################\n",
    "import sys\n",
    "import collections\n",
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import mode\n",
    "\n",
    "plt.style.use('bmh')\n",
    "%matplotlib inline\n",
    "\n",
    "try:\n",
    "    from IPython.display import clear_output\n",
    "    have_ipython = True\n",
    "except ImportError:\n",
    "    have_ipython = False\n",
    "\n",
    "class KnnDtw(object):\n",
    "    \"\"\"K-nearest neighbor classifier using dynamic time warping\n",
    "    as the distance measure between pairs of time series arrays\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    n_neighbors : int, optional (default = 5)\n",
    "        Number of neighbors to use by default for KNN\n",
    "        \n",
    "    max_warping_window : int, optional (default = infinity)\n",
    "        Maximum warping window allowed by the DTW dynamic\n",
    "        programming function\n",
    "            \n",
    "    subsample_step : int, optional (default = 1)\n",
    "        Step size for the timeseries array. By setting subsample_step = 2,\n",
    "        the timeseries length will be reduced by 50% because every second\n",
    "        item is skipped. Implemented by x[:, ::subsample_step]\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_neighbors=5, max_warping_window=10000, subsample_step=1):\n",
    "        self.n_neighbors = n_neighbors\n",
    "        self.max_warping_window = max_warping_window\n",
    "        self.subsample_step = subsample_step\n",
    "    \n",
    "    def fit(self, x, l):\n",
    "        \"\"\"Fit the model using x as training data and l as class labels\n",
    "        \n",
    "        Arguments\n",
    "        ---------\n",
    "        x : array of shape [n_samples, n_timepoints]\n",
    "            Training data set for input into KNN classifer\n",
    "            \n",
    "        l : array of shape [n_samples]\n",
    "            Training labels for input into KNN classifier\n",
    "        \"\"\"\n",
    "        \n",
    "        self.x = x\n",
    "        self.l = l\n",
    "        \n",
    "    def _dtw_distance(self, ts_a, ts_b, d = lambda x,y: abs(x-y)):\n",
    "        \"\"\"Returns the DTW similarity distance between two 2-D\n",
    "        timeseries numpy arrays.\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        ts_a, ts_b : array of shape [n_samples, n_timepoints]\n",
    "            Two arrays containing n_samples of timeseries data\n",
    "            whose DTW distance between each sample of A and B\n",
    "            will be compared\n",
    "        \n",
    "        d : DistanceMetric object (default = abs(x-y))\n",
    "            the distance measure used for A_i - B_j in the\n",
    "            DTW dynamic programming function\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        DTW distance between A and B\n",
    "        \"\"\"\n",
    "\n",
    "        # Create cost matrix via broadcasting with large int\n",
    "        ts_a, ts_b = np.array(ts_a), np.array(ts_b)\n",
    "        M, N = len(ts_a), len(ts_b)\n",
    "        cost = sys.maxsize * np.ones((M, N))\n",
    "\n",
    "        # Initialize the first row and column\n",
    "        cost[0, 0] = d(ts_a[0], ts_b[0])\n",
    "        for i in range(1, M):\n",
    "            cost[i, 0] = cost[i-1, 0] + d(ts_a[i], ts_b[0])\n",
    "\n",
    "        for j in range(1, N):\n",
    "            cost[0, j] = cost[0, j-1] + d(ts_a[0], ts_b[j])\n",
    "\n",
    "        # Populate rest of cost matrix within window\n",
    "        for i in range(1, M):\n",
    "            for j in range(max(1, i - self.max_warping_window),\n",
    "                            min(N, i + self.max_warping_window)):\n",
    "                choices = cost[i - 1, j - 1], cost[i, j-1], cost[i-1, j]\n",
    "                cost[i, j] = min(choices) + d(ts_a[i], ts_b[j])\n",
    "\n",
    "        # Return DTW distance given window \n",
    "        return cost[-1, -1]\n",
    "    \n",
    "    def _dist_matrix(self, x, y):\n",
    "        \"\"\"Computes the M x N distance matrix between the training\n",
    "        dataset and testing dataset (y) using the DTW distance measure\n",
    "        \n",
    "        Arguments\n",
    "        ---------\n",
    "        x : array of shape [n_samples, n_timepoints]\n",
    "        \n",
    "        y : array of shape [n_samples, n_timepoints]\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Distance matrix between each item of x and y with\n",
    "            shape [training_n_samples, testing_n_samples]\n",
    "        \"\"\"\n",
    "        \n",
    "        # Compute the distance matrix        \n",
    "        dm_count = 0\n",
    "        \n",
    "        # Compute condensed distance matrix (upper triangle) of pairwise dtw distances\n",
    "        # when x and y are the same array\n",
    "        if(np.array_equal(x, y)):\n",
    "            x_s = shape(x)\n",
    "            dm = np.zeros((x_s[0] * (x_s[0] - 1)) // 2, dtype=np.double)\n",
    "            \n",
    "            #p = ProgressBar(shape(dm)[0])\n",
    "            \n",
    "            for i in range(0, x_s[0] - 1):\n",
    "                for j in range(i + 1, x_s[0]):\n",
    "                    dm[dm_count] = self._dtw_distance(x[i, ::self.subsample_step],\n",
    "                                                      y[j, ::self.subsample_step])\n",
    "                    \n",
    "                    dm_count += 1\n",
    "                    #p.animate(dm_count)\n",
    "            \n",
    "            # Convert to squareform\n",
    "            dm = squareform(dm)\n",
    "            return dm\n",
    "        \n",
    "        # Compute full distance matrix of dtw distnces between x and y\n",
    "        else:\n",
    "            x_s = np.shape(x)\n",
    "            y_s = np.shape(y)\n",
    "            dm = np.zeros((x_s[0], y_s[0])) \n",
    "            dm_size = x_s[0]*y_s[0]\n",
    "            \n",
    "            #p = ProgressBar(dm_size)\n",
    "        \n",
    "            for i in range(0, x_s[0]):\n",
    "                for j in range(0, y_s[0]):\n",
    "                    dm[i, j] = self._dtw_distance(x[i, ::self.subsample_step],\n",
    "                                                  y[j, ::self.subsample_step])\n",
    "                    # Update progress bar\n",
    "                    dm_count += 1\n",
    "                    #p.animate(dm_count)\n",
    "            return dm\n",
    "        \n",
    "    def predict(self, x):\n",
    "        \"\"\"Predict the class labels or probability estimates for \n",
    "        the provided data\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "          x : array of shape [n_samples, n_timepoints]\n",
    "              Array containing the testing data set to be classified\n",
    "          \n",
    "        Returns\n",
    "        -------\n",
    "          2 arrays representing:\n",
    "              (1) the predicted class labels \n",
    "              (2) the knn label count probability\n",
    "        \"\"\"\n",
    "        \n",
    "        dm = self._dist_matrix(x, self.x)\n",
    "\n",
    "        # Identify the k nearest neighbors\n",
    "        knn_idx = dm.argsort()[:, :self.n_neighbors]\n",
    "\n",
    "        # Identify k nearest labels\n",
    "        knn_labels = self.l[knn_idx]\n",
    "        \n",
    "        # Model Label\n",
    "        mode_data = mode(knn_labels, axis=1)\n",
    "        mode_label = mode_data[0]\n",
    "        mode_proba = mode_data[1]/self.n_neighbors\n",
    "\n",
    "        return mode_label.ravel(), mode_proba.ravel()\n",
    "\n",
    "class ProgressBar:\n",
    "    \"\"\"This progress bar was taken from PYMC\n",
    "    \"\"\"\n",
    "    def __init__(self, iterations):\n",
    "        self.iterations = iterations\n",
    "        self.prog_bar = '[]'\n",
    "        self.fill_char = '*'\n",
    "        self.width = 40\n",
    "        self.__update_amount(0)\n",
    "        if have_ipython:\n",
    "            self.animate = self.animate_ipython\n",
    "        else:\n",
    "            self.animate = self.animate_noipython\n",
    "\n",
    "    def animate_ipython(self, iter):\n",
    "        sys.stdout.write('\\r%s'%self)\n",
    "        sys.stdout.flush()\n",
    "        self.update_iteration(iter + 1)\n",
    "\n",
    "    def update_iteration(self, elapsed_iter):\n",
    "        self.__update_amount((elapsed_iter / float(self.iterations)) * 100.0)\n",
    "        self.prog_bar += '  %d of %s complete' % (elapsed_iter, self.iterations)\n",
    "\n",
    "    def __update_amount(self, new_amount):\n",
    "        percent_done = int(round((new_amount / 100.0) * 100.0))\n",
    "        all_full = self.width - 2\n",
    "        num_hashes = int(round((percent_done / 100.0) * all_full))\n",
    "        self.prog_bar = '[' + self.fill_char * num_hashes + ' ' * (all_full - num_hashes) + ']'\n",
    "        pct_place = (len(self.prog_bar) // 2) - len(str(percent_done))\n",
    "        pct_string = '%d%%' % percent_done\n",
    "        self.prog_bar = self.prog_bar[0:pct_place] + \\\n",
    "            (pct_string + self.prog_bar[pct_place + len(pct_string):])\n",
    "\n",
    "    def __str__(self):\n",
    "        return str(self.prog_bar)\n",
    "\n",
    "######################################################################################################################\n",
    "\n",
    "\n",
    "import tensorflow as tf \n",
    "import keras \n",
    "import keras.backend as K\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
    "from collections import Counter\n",
    "\n",
    "from keras import regularizers\n",
    "from keras.models import Sequential, Model, load_model, model_from_json \n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Input, Dense, Flatten, Reshape, Concatenate,  Dropout \n",
    "from keras.layers import Conv2D, MaxPooling2D, UpSampling2D, Conv2DTranspose\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from scipy.signal import resample\n",
    "\n",
    "\n",
    "################################################################################\n",
    "def get_ds_infos():\n",
    "    \"\"\"\n",
    "    Read the file includes data subject information.\n",
    "    \n",
    "    Data Columns:\n",
    "    0: code [1-24]\n",
    "    1: weight [kg]\n",
    "    2: height [cm]\n",
    "    3: age [years]\n",
    "    4: gender [0:Female, 1:Male]\n",
    "    \n",
    "    Returns:\n",
    "        A pandas DataFrame that contains inforamtion about data subjects' attributes \n",
    "    \"\"\" \n",
    "\n",
    "    dss = pd.read_csv(\"data_subjects_info.csv\")\n",
    "    print(\"[INFO] -- Data subjects' information is imported.\")\n",
    "    \n",
    "    return dss\n",
    "\n",
    "def set_data_types(data_types=[\"userAcceleration\"]):\n",
    "    \"\"\"\n",
    "    Select the sensors and the mode to shape the final dataset.\n",
    "    \n",
    "    Args:\n",
    "        data_types: A list of sensor data type from this list: [attitude, gravity, rotationRate, userAcceleration] \n",
    "\n",
    "    Returns:\n",
    "        It returns a list of columns to use for creating time-series from files.\n",
    "    \"\"\"\n",
    "    dt_list = []\n",
    "    for t in data_types:\n",
    "        if t != \"attitude\":\n",
    "            dt_list.append([t+\".x\",t+\".y\",t+\".z\"])\n",
    "        else:\n",
    "            dt_list.append([t+\".roll\", t+\".pitch\", t+\".yaw\"])\n",
    "\n",
    "    return dt_list\n",
    "\n",
    "\n",
    "def creat_time_series(dt_list, act_labels, trial_codes, mode=\"mag\", labeled=True, combine_grav_acc=False):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        dt_list: A list of columns that shows the type of data we want.\n",
    "        act_labels: list of activites\n",
    "        trial_codes: list of trials\n",
    "        mode: It can be \"raw\" which means you want raw data\n",
    "        for every dimention of each data type,\n",
    "        [attitude(roll, pitch, yaw); gravity(x, y, z); rotationRate(x, y, z); userAcceleration(x,y,z)].\n",
    "        or it can be \"mag\" which means you only want the magnitude for each data type: (x^2+y^2+z^2)^(1/2)\n",
    "        labeled: True, if we want a labeld dataset. False, if we only want sensor values.\n",
    "        combine_grav_acc: True, means adding each axis of gravity to  corresponding axis of userAcceleration.\n",
    "    Returns: \n",
    "        It returns a time-series of sensor data.\n",
    "    \n",
    "    \"\"\"\n",
    "    num_data_cols = len(dt_list) if mode == \"mag\" else len(dt_list*3)\n",
    "\n",
    "    if labeled:\n",
    "        dataset = np.zeros((0,num_data_cols+7)) # \"7\" --> [act, code, weight, height, age, gender, trial] \n",
    "    else:\n",
    "        dataset = np.zeros((0,num_data_cols))\n",
    "        \n",
    "    ds_list = get_ds_infos()\n",
    "    \n",
    "    print(\"[INFO] -- Creating Time-Series\")\n",
    "    for sub_id in ds_list[\"code\"]:\n",
    "        for act_id, act in enumerate(act_labels):\n",
    "            for trial in trial_codes[act_id]:\n",
    "                fname = 'A_DeviceMotion_data/'+act+'_'+str(trial)+'/sub_'+str(int(sub_id))+'.csv'\n",
    "                raw_data = pd.read_csv(fname)\n",
    "                raw_data = raw_data.drop(['Unnamed: 0'], axis=1)\n",
    "                vals = np.zeros((len(raw_data), num_data_cols))\n",
    "                \n",
    "                if combine_grav_acc:\n",
    "                    raw_data[\"userAcceleration.x\"] = raw_data[\"userAcceleration.x\"].add(raw_data[\"gravity.x\"])\n",
    "                    raw_data[\"userAcceleration.y\"] = raw_data[\"userAcceleration.y\"].add(raw_data[\"gravity.y\"])\n",
    "                    raw_data[\"userAcceleration.z\"] = raw_data[\"userAcceleration.z\"].add(raw_data[\"gravity.z\"])\n",
    "                \n",
    "                for x_id, axes in enumerate(dt_list):\n",
    "                    if mode == \"mag\":\n",
    "                        vals[:,x_id] = (raw_data[axes]**2).sum(axis=1)**0.5        \n",
    "                    else:\n",
    "                        vals[:,x_id*3:(x_id+1)*3] = raw_data[axes].values\n",
    "                    vals = vals[:,:num_data_cols]\n",
    "                if labeled:\n",
    "                    lbls = np.array([[act_id,\n",
    "                            sub_id-1,\n",
    "                            ds_list[\"weight\"][sub_id-1],\n",
    "                            ds_list[\"height\"][sub_id-1],\n",
    "                            ds_list[\"age\"][sub_id-1],\n",
    "                            ds_list[\"gender\"][sub_id-1],\n",
    "                            trial          \n",
    "                           ]]*len(raw_data))\n",
    "                    vals = np.concatenate((vals, lbls), axis=1)\n",
    "                dataset = np.append(dataset,vals, axis=0)\n",
    "    cols = []\n",
    "    for axes in dt_list:\n",
    "        if mode == \"raw\":\n",
    "            cols += axes\n",
    "        else:\n",
    "            cols += [str(axes[0][:-2])]\n",
    "            \n",
    "    if labeled:\n",
    "        cols += [\"act\", \"id\", \"weight\", \"height\", \"age\", \"gender\", \"trial\"]\n",
    "    \n",
    "    dataset = pd.DataFrame(data=dataset, columns=cols)\n",
    "    return dataset\n",
    "#________________________________\n",
    "#________________________________\n",
    "\n",
    "def ts_to_secs(dataset, w, s, standardize = False, **options):\n",
    "    \n",
    "    data = dataset[dataset.columns[:-7]].values    \n",
    "    act_labels = dataset[\"act\"].values\n",
    "    id_labels = dataset[\"id\"].values\n",
    "    trial_labels = dataset[\"trial\"].values\n",
    "\n",
    "    mean = 0\n",
    "    std = 1\n",
    "    if standardize:\n",
    "        ## Standardize each sensor’s data to have a zero mean and unity standard deviation.\n",
    "        ## As usual, we normalize test dataset by training dataset's parameters \n",
    "        if options:\n",
    "            mean = options.get(\"mean\")\n",
    "            std = options.get(\"std\")\n",
    "            print(\"[INFO] -- Test Data has been standardized\")\n",
    "        else:\n",
    "            mean = data.mean(axis=0)\n",
    "            std = data.std(axis=0)\n",
    "            print(\"[INFO] -- Training Data has been standardized: the mean is = \"+str(mean)+\" ; and the std is = \"+str(std))            \n",
    "\n",
    "        data -= mean\n",
    "        data /= std\n",
    "    else:\n",
    "        print(\"[INFO] -- Without Standardization.....\")\n",
    "\n",
    "    ## We want the Rows of matrices show each Feature and the Columns show time points.\n",
    "    data = data.T\n",
    "\n",
    "    m = data.shape[0]   # Data Dimension \n",
    "    ttp = data.shape[1] # Total Time Points\n",
    "    number_of_secs = int(round(((ttp - w)/s)))\n",
    "\n",
    "    ##  Create a 3D matrix for Storing Sections  \n",
    "    secs_data = np.zeros((number_of_secs , m , w ))\n",
    "    act_secs_labels = np.zeros(number_of_secs)\n",
    "    id_secs_labels = np.zeros(number_of_secs)\n",
    "\n",
    "    k=0\n",
    "    for i in range(0 , ttp-w, s):\n",
    "        j = i // s\n",
    "        if j >= number_of_secs:\n",
    "            break\n",
    "        if id_labels[i] != id_labels[i+w-1]: \n",
    "            continue\n",
    "        if act_labels[i] != act_labels[i+w-1]: \n",
    "            continue\n",
    "        if trial_labels[i] != trial_labels[i+w-1]:\n",
    "            continue\n",
    "            \n",
    "        secs_data[k] = data[:, i:i+w]\n",
    "        act_secs_labels[k] = act_labels[i].astype(int)\n",
    "        id_secs_labels[k] = id_labels[i].astype(int)\n",
    "        k = k+1\n",
    "        \n",
    "    secs_data = secs_data[0:k]\n",
    "    act_secs_labels = act_secs_labels[0:k]\n",
    "    id_secs_labels = id_secs_labels[0:k]\n",
    "    return secs_data, act_secs_labels, id_secs_labels, mean, std\n",
    "##________________________________________________________________\n",
    "\n",
    "\n",
    "ACT_LABELS = [\"dws\",\"ups\", \"wlk\", \"jog\", \"std\", \"sit\"]\n",
    "TRIAL_CODES = {\n",
    "    ACT_LABELS[0]:[1,2,11],\n",
    "    ACT_LABELS[1]:[3,4,12],\n",
    "    ACT_LABELS[2]:[7,8,15],\n",
    "    ACT_LABELS[3]:[9,16],\n",
    "    ACT_LABELS[4]:[6,14],\n",
    "    ACT_LABELS[5]:[5,13],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] -- Selected sensor data types: ['rotationRate', 'userAcceleration'] -- Mode: mag -- Grav+Acc: True\n",
      "[INFO] -- Selected activites: ['dws', 'ups', 'wlk', 'jog']\n",
      "[INFO] -- Data subjects' information is imported.\n",
      "[INFO] -- Creating Time-Series\n",
      "[INFO] -- Shape of time-Series dataset:(767660, 9)\n",
      "[INFO] -- Test IDs: [4, 9, 11, 21]\n",
      "[INFO] -- Shape of Train Time-Series :(646207, 9)\n",
      "[INFO] -- Shape of Test Time-Series :(121453, 9)\n"
     ]
    }
   ],
   "source": [
    "## Here we set parameter to build labeld time-series from dataset of \"(A)DeviceMotion_data\"\n",
    "## attitude(roll, pitch, yaw); gravity(x, y, z); rotationRate(x, y, z); userAcceleration(x,y,z)\n",
    "sdt = [\"rotationRate\",\"userAcceleration\"]\n",
    "mode = \"mag\"\n",
    "cga = True # Add gravity to acceleration or not\n",
    "print(\"[INFO] -- Selected sensor data types: \"+str(sdt)+\" -- Mode: \"+str(mode)+\" -- Grav+Acc: \"+str(cga))    \n",
    "\n",
    "act_labels = ACT_LABELS [0:4]\n",
    "print(\"[INFO] -- Selected activites: \"+str(act_labels))    \n",
    "trial_codes = [TRIAL_CODES[act] for act in act_labels]\n",
    "dt_list = set_data_types(sdt)\n",
    "dataset = creat_time_series(dt_list, act_labels, trial_codes, mode=mode, labeled=True, combine_grav_acc = cga)\n",
    "print(\"[INFO] -- Shape of time-Series dataset:\"+str(dataset.shape))    \n",
    "\n",
    "#*****************\n",
    "TRAIN_TEST_TYPE = \"subject\" # \"subject\" or \"trial\"\n",
    "#*****************\n",
    "\n",
    "if TRAIN_TEST_TYPE == \"subject\":\n",
    "    test_ids = [4,9,11,21]\n",
    "    print(\"[INFO] -- Test IDs: \"+str(test_ids))\n",
    "    test_ts = dataset.loc[(dataset['id'].isin(test_ids))]\n",
    "    train_ts = dataset.loc[~(dataset['id'].isin(test_ids))]\n",
    "else:\n",
    "    test_trail = [11,12,13,14,15,16]  \n",
    "    print(\"[INFO] -- Test Trials: \"+str(test_trail))\n",
    "    test_ts = dataset.loc[(dataset['trial'].isin(test_trail))]\n",
    "    train_ts = dataset.loc[~(dataset['trial'].isin(test_trail))]\n",
    "\n",
    "    \n",
    "print(\"[INFO] -- Shape of Train Time-Series :\"+str(train_ts.shape))\n",
    "print(\"[INFO] -- Shape of Test Time-Series :\"+str(test_ts.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] -- Shape of Train Time-Series :(523129, 9)\n",
      "[INFO] -- Shape of Test Time-Series :(123078, 9)\n"
     ]
    }
   ],
   "source": [
    "val_trail = [11,12,13,14,15,16]\n",
    "val_ts = train_ts.loc[(train_ts['trial'].isin(val_trail))]\n",
    "train_ts = train_ts.loc[~(train_ts['trial'].isin(val_trail))]\n",
    "print(\"[INFO] -- Shape of Train Time-Series :\"+str(train_ts.shape))\n",
    "print(\"[INFO] -- Shape of Test Time-Series :\"+str(val_ts.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] -- Training Data has been standardized: the mean is = [2.17728825 1.19431016] ; and the std is = [1.43229632 0.70168121]\n",
      "[INFO] -- Test Data has been standardized\n",
      "[INFO] -- Test Data has been standardized\n",
      "[INFO] -- Shape of Training Sections: (50532, 2, 128)\n",
      "[INFO] -- Shape of Training Sections: (11288, 2, 128)\n",
      "[INFO] -- Shape of Test Sections:  (11589, 2, 128)\n"
     ]
    }
   ],
   "source": [
    "#************\n",
    "## HERE ##\n",
    "\n",
    "## This Variable Defines the Size of Sliding Window\n",
    "## ( e.g. 100 means in each snapshot we just consider 100 consecutive observations of each sensor) \n",
    "w = 128 # 50 Equals to 1 second for MotionSense Dataset (it is on 50Hz samplig rate)\n",
    "## Here We Choose Step Size for Building Diffrent Snapshots from Time-Series Data\n",
    "## ( smaller step size will increase the amount of the instances and higher computational cost may be incurred )\n",
    "s = 10\n",
    "train_data, act_train, id_train, train_mean, train_std = ts_to_secs(train_ts.copy(),\n",
    "                                                                   w,\n",
    "                                                                   s,\n",
    "                                                                   standardize = True)\n",
    "\n",
    "s = 10\n",
    "val_data, act_val, id_val, val_mean, val_std = ts_to_secs(val_ts.copy(),\n",
    "                                                              w,\n",
    "                                                              s,\n",
    "                                                              standardize = True,\n",
    "                                                              mean = train_mean, \n",
    "                                                              std = train_std)\n",
    "\n",
    "\n",
    "s = 10\n",
    "test_data, act_test, id_test, test_mean, test_std = ts_to_secs(test_ts.copy(),\n",
    "                                                              w,\n",
    "                                                              s,\n",
    "                                                              standardize = True,\n",
    "                                                              mean = train_mean, \n",
    "                                                              std = train_std)\n",
    "\n",
    "print(\"[INFO] -- Shape of Training Sections: \"+str(train_data.shape))\n",
    "print(\"[INFO] -- Shape of Training Sections: \"+str(val_data.shape))\n",
    "print(\"[INFO] -- Shape of Test Sections:  \"+str(test_data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] -- Shape of Training Sections: (50532, 2, 128, 1)\n",
      "[INFO] -- Validation Sections:(11288, 2, 128, 1)\n",
      "[INFO] -- Shape of Training Sections: (11589, 2, 128, 1)\n"
     ]
    }
   ],
   "source": [
    "id_train_labels = to_categorical(id_train)\n",
    "id_val_labels = to_categorical(id_val)\n",
    "id_test_labels = to_categorical(id_test)\n",
    "id_test_labels = np.append(id_test_labels, np.zeros((len(id_test_labels),2)), axis =1)\n",
    "\n",
    "act_train_labels = to_categorical(act_train)\n",
    "act_val_labels = to_categorical(act_val)\n",
    "act_test_labels = to_categorical(act_test)\n",
    "    \n",
    "## Here we add an extra dimension to the datasets just to be ready for using with Convolution2D\n",
    "train_data = np.expand_dims(train_data,axis=3)\n",
    "print(\"[INFO] -- Shape of Training Sections:\", train_data.shape)\n",
    "val_data = np.expand_dims(val_data,axis=3)\n",
    "print(\"[INFO] -- Validation Sections:\"+str(val_data.shape))\n",
    "test_data = np.expand_dims(test_data,axis=3)\n",
    "print(\"[INFO] -- Shape of Training Sections:\", test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Sampels = 25\n",
      "[INFO] -- Downsampled Test Sections: (11589, 2, 25, 1)\n",
      "[INFO] -- Test Sections: (11589, 2, 128, 1)\n",
      "(11589, 2, 128, 1)\n"
     ]
    }
   ],
   "source": [
    "Xraw = train_data.copy()\n",
    "tXraw = test_data.copy()\n",
    "test_ids = [4,9,11,21]\n",
    "#*******************\n",
    "sample_rate = 10 #Hz\n",
    "#*******************\n",
    "num_sampels = (128*sample_rate)//50\n",
    "print(\"Number of Sampels = \"+str(num_sampels))\n",
    "tX10 = tXraw.copy()\n",
    "\n",
    "from scipy.signal import resample\n",
    "ds_test_data = tX10.copy()\n",
    "\n",
    "for sens in range(2):\n",
    "    tmp = np.array([resample(x,num_sampels) for x in ds_test_data[:,sens,:,0]])\n",
    "    ds_test_data[:,sens,:num_sampels,0] = tmp \n",
    "\n",
    "ds_test_data = ds_test_data[:,:,:num_sampels,:]\n",
    "print(\"[INFO] -- Downsampled Test Sections:\", ds_test_data.shape)\n",
    "\n",
    "for sens in range(2):\n",
    "    tmp = np.array([resample(x,128) for x in ds_test_data[:,sens,:,0]])\n",
    "    tX10[:,sens,:,0] = tmp \n",
    "    \n",
    "print(\"[INFO] -- Test Sections:\", tX10.shape)\n",
    "print(tX10.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(242, 2, 128, 1)\n",
      "Test User: 4\n",
      "j: 23\n",
      "Rank: 2.0\n",
      "Test User: 9\n",
      "j: 23\n",
      "Rank: 5.0\n",
      "Test User: 11\n",
      "j: 23\n",
      "Rank: 15.0\n",
      "Test User: 21\n",
      "j: 23\n",
      "Rank: 11.0\n",
      "{2: 1, 19: 3}\n",
      "[ 2.  5. 15. 11.]\n",
      "Mean: 8.25 -- Std = 5.0682837331783235\n",
      "Test User: 4\n",
      "j: 23\n",
      "Rank: 0.0\n",
      "Test User: 9\n",
      "j: 23\n",
      "Rank: 12.0\n",
      "Test User: 11\n",
      "j: 23\n",
      "Rank: 2.0\n",
      "Test User: 21\n",
      "j: 23\n",
      "Rank: 13.0\n",
      "{2: 1, 19: 3, 4: 1, 10: 1, 22: 2}\n",
      "[ 0. 12.  2. 13.]\n",
      "Mean: 7.5 -- Std = 5.436188558150137\n",
      "Test User: 4\n",
      "j: 23\n",
      "Rank: 0.0\n",
      "Test User: 9\n",
      "j: 23\n",
      "Rank: 14.0\n",
      "Test User: 11\n",
      "j: 23\n",
      "Rank: 15.0\n",
      "Test User: 21\n",
      "j: 23\n",
      "Rank: 5.0\n",
      "{2: 1, 19: 3, 4: 2, 22: 4, 20: 1, 10: 1}\n",
      "[ 0. 14. 15.  5.]\n",
      "Mean: 7.833333333333333 -- Std = 5.712453053123703\n",
      "Test User: 4\n",
      "j: 23\n",
      "Rank: 0.0\n",
      "Test User: 9\n",
      "j: 23\n",
      "Rank: 14.0\n",
      "Test User: 11\n",
      "j: 23\n",
      "Rank: 7.0\n",
      "Test User: 21\n",
      "j: 23\n",
      "Rank: 0.0\n",
      "{2: 1, 19: 3, 4: 5, 21: 1, 22: 4, 20: 1, 10: 1}\n",
      "[ 0. 14.  7.  0.]\n",
      "Mean: 7.1875 -- Std = 5.735363135623265\n",
      "Final Mean: 7.1875 -- Final Std = 5.735363135623265\n"
     ]
    }
   ],
   "source": [
    "dist = np.zeros((4,24))\n",
    "ranks = np.zeros(4)\n",
    "kwd = KnnDtw(n_neighbors=10, subsample_step=2)\n",
    "nn = {} \n",
    "mean = 0\n",
    "std = 0\n",
    "dd = (w//s)*4\n",
    "\n",
    "## Labels\n",
    "rdu_act_X = act_train_labels[::dd]\n",
    "rdu_act_tX = act_test_labels[::dd]\n",
    "rdu_id_X = id_train_labels[::dd]\n",
    "rdu_id_tX = id_test_labels[::dd]\n",
    "\n",
    "# Test Data: after Transformation\n",
    "rdu_tX10 = tX10[::dd]\n",
    "print(rdu_tX10.shape)\n",
    "\n",
    "\n",
    "# Training and Test Data: Raw\n",
    "rdu_Xraw = Xraw[::dd]\n",
    "rdu_tXraw = tXraw[::dd]\n",
    "rdu_all_Xraw = np.append(rdu_Xraw, rdu_tXraw, axis=0)\n",
    "rdu_all_act = np.append(rdu_act_X,rdu_act_tX,axis=0)\n",
    "rdu_all_id = np.append(rdu_id_X,rdu_id_tX,axis=0)\n",
    "\n",
    "\n",
    "import sys\n",
    "# just considering one activity, like walking. We want to compare data acrros users not activites. \n",
    "for act in range(4):\n",
    "    dic = {}\n",
    "    for i in range(len(test_ids)):\n",
    "        print(\"Test User: \"+str(test_ids[i]))\n",
    "        # Transformed Data\n",
    "        Xhi = rdu_tX10[np.logical_and(rdu_id_tX[:,test_ids[i]]==1., rdu_act_tX[:,act]==1.)]\n",
    "        for j in range(24):\n",
    "            # Raw Data\n",
    "            Xj = rdu_all_Xraw[np.logical_and(rdu_all_id[:,j]==1., rdu_all_act[:,act]==1.)]\n",
    "            mat = kwd._dist_matrix(Xhi[:,0,:,0], Xj[:,0,:,0])\n",
    "            s0 = (mat.sum())/(len(Xhi)*len(Xj))\n",
    "            \n",
    "            mat = kwd._dist_matrix(Xhi[:,1,:,0], Xj[:,1,:,0])\n",
    "            s1 = (mat.sum())/(len(Xhi)*len(Xj))\n",
    "            dist[i,j] = (s0+s1)/2.\n",
    "            \n",
    "            sys.stdout.write(\"\\rj: \"+str(j))\n",
    "            sys.stdout.flush()\n",
    "\n",
    "        print(\"\")    \n",
    "        tmp = np.argsort(dist[i])\n",
    "        tmp = np.where(tmp == test_ids[i])\n",
    "        ranks[i] = tmp[0][0]\n",
    "        print(\"Rank: \"+str(ranks[i]))\n",
    "        if np.argsort(dist[i])[0] in nn:\n",
    "            nn[np.argsort(dist[i])[0]] += 1\n",
    "        else:\n",
    "            nn[np.argsort(dist[i])[0]] = 1 \n",
    "    print(nn)\n",
    "    print(ranks)  \n",
    "    mean += ranks.mean()\n",
    "    std +=ranks.std()\n",
    "    print(\"Mean: \"+str(mean/(act+1))+\" -- Std = \"+str(std/(act+1)))\n",
    "mean = mean/4.\n",
    "std = std/4.\n",
    "print(\"Final Mean: \"+str(mean)+\" -- Final Std = \"+str(std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Sampels = 12\n",
      "[INFO] -- Downsampled Test Sections: (11589, 2, 12, 1)\n",
      "[INFO] -- Test Sections: (11589, 2, 128, 1)\n",
      "(11589, 2, 128, 1)\n"
     ]
    }
   ],
   "source": [
    "Xraw = train_data.copy()\n",
    "tXraw = test_data.copy()\n",
    "test_ids = [4,9,11,21]\n",
    "#*******************\n",
    "sample_rate = 5 #Hz\n",
    "#*******************\n",
    "num_sampels = (128*sample_rate)//50\n",
    "print(\"Number of Sampels = \"+str(num_sampels))\n",
    "tX10 = tXraw.copy()\n",
    "\n",
    "from scipy.signal import resample\n",
    "ds_test_data = tX10.copy()\n",
    "\n",
    "for sens in range(2):\n",
    "    tmp = np.array([resample(x,num_sampels) for x in ds_test_data[:,sens,:,0]])\n",
    "    ds_test_data[:,sens,:num_sampels,0] = tmp \n",
    "\n",
    "ds_test_data = ds_test_data[:,:,:num_sampels,:]\n",
    "print(\"[INFO] -- Downsampled Test Sections:\", ds_test_data.shape)\n",
    "\n",
    "for sens in range(2):\n",
    "    tmp = np.array([resample(x,128) for x in ds_test_data[:,sens,:,0]])\n",
    "    tX10[:,sens,:,0] = tmp \n",
    "    \n",
    "print(\"[INFO] -- Test Sections:\", tX10.shape)\n",
    "print(tX10.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(242, 2, 128, 1)\n",
      "Test User: 4\n",
      "j: 23\n",
      "Rank: 2.0\n",
      "Test User: 9\n",
      "j: 23\n",
      "Rank: 6.0\n",
      "Test User: 11\n",
      "j: 23\n",
      "Rank: 16.0\n",
      "Test User: 21\n",
      "j: 23\n",
      "Rank: 12.0\n",
      "{2: 2, 19: 2}\n",
      "[ 2.  6. 16. 12.]\n",
      "Mean: 9.0 -- Std = 5.385164807134504\n",
      "Test User: 4\n",
      "j: 23\n",
      "Rank: 1.0\n",
      "Test User: 9\n",
      "j: 23\n",
      "Rank: 15.0\n",
      "Test User: 11\n",
      "j: 23\n",
      "Rank: 6.0\n",
      "Test User: 21\n",
      "j: 23\n",
      "Rank: 18.0\n",
      "{22: 1, 2: 2, 19: 2, 10: 1, 6: 2}\n",
      "[ 1. 15.  6. 18.]\n",
      "Mean: 9.5 -- Std = 6.102127827813716\n",
      "Test User: 4\n",
      "j: 23\n",
      "Rank: 7.0\n",
      "Test User: 9\n",
      "j: 23\n",
      "Rank: 14.0\n",
      "Test User: 11\n",
      "j: 23\n",
      "Rank: 14.0\n",
      "Test User: 21\n",
      "j: 23\n",
      "Rank: 6.0\n",
      "{2: 2, 19: 2, 20: 1, 6: 3, 22: 3, 10: 1}\n",
      "[ 7. 14. 14.  6.]\n",
      "Mean: 9.75 -- Std = 5.323628482985757\n",
      "Test User: 4\n",
      "j: 23\n",
      "Rank: 5.0\n",
      "Test User: 9\n",
      "j: 23\n",
      "Rank: 20.0\n",
      "Test User: 11\n",
      "j: 23\n",
      "Rank: 7.0\n",
      "Test User: 21\n",
      "j: 23\n",
      "Rank: 0.0\n",
      "{2: 2, 19: 2, 20: 1, 6: 3, 22: 3, 10: 1, 18: 1, 21: 3}\n",
      "[ 5. 20.  7.  0.]\n",
      "Mean: 9.3125 -- Std = 5.8383242447684935\n",
      "Final Mean: 9.3125 -- Final Std = 5.8383242447684935\n"
     ]
    }
   ],
   "source": [
    "dist = np.zeros((4,24))\n",
    "ranks = np.zeros(4)\n",
    "kwd = KnnDtw(n_neighbors=10, subsample_step=2)\n",
    "nn = {} \n",
    "mean = 0\n",
    "std = 0\n",
    "dd = (w//s)*4\n",
    "\n",
    "## Labels\n",
    "rdu_act_X = act_train_labels[::dd]\n",
    "rdu_act_tX = act_test_labels[::dd]\n",
    "rdu_id_X = id_train_labels[::dd]\n",
    "rdu_id_tX = id_test_labels[::dd]\n",
    "\n",
    "# Test Data: after Transformation\n",
    "rdu_tX10 = tX10[::dd]\n",
    "print(rdu_tX10.shape)\n",
    "\n",
    "\n",
    "# Training and Test Data: Raw\n",
    "rdu_Xraw = Xraw[::dd]\n",
    "rdu_tXraw = tXraw[::dd]\n",
    "rdu_all_Xraw = np.append(rdu_Xraw, rdu_tXraw, axis=0)\n",
    "rdu_all_act = np.append(rdu_act_X,rdu_act_tX,axis=0)\n",
    "rdu_all_id = np.append(rdu_id_X,rdu_id_tX,axis=0)\n",
    "\n",
    "\n",
    "import sys\n",
    "# just considering one activity, like walking. We want to compare data acrros users not activites. \n",
    "for act in range(4):\n",
    "    dic = {}\n",
    "    for i in range(len(test_ids)):\n",
    "        print(\"Test User: \"+str(test_ids[i]))\n",
    "        # Transformed Data\n",
    "        Xhi = rdu_tX10[np.logical_and(rdu_id_tX[:,test_ids[i]]==1., rdu_act_tX[:,act]==1.)]\n",
    "        for j in range(24):\n",
    "            # Raw Data\n",
    "            Xj = rdu_all_Xraw[np.logical_and(rdu_all_id[:,j]==1., rdu_all_act[:,act]==1.)]\n",
    "            mat = kwd._dist_matrix(Xhi[:,0,:,0], Xj[:,0,:,0])\n",
    "            s0 = (mat.sum())/(len(Xhi)*len(Xj))\n",
    "            \n",
    "            mat = kwd._dist_matrix(Xhi[:,1,:,0], Xj[:,1,:,0])\n",
    "            s1 = (mat.sum())/(len(Xhi)*len(Xj))\n",
    "            dist[i,j] = (s0+s1)/2.\n",
    "            \n",
    "            sys.stdout.write(\"\\rj: \"+str(j))\n",
    "            sys.stdout.flush()\n",
    "\n",
    "        print(\"\")    \n",
    "        tmp = np.argsort(dist[i])\n",
    "        tmp = np.where(tmp == test_ids[i])\n",
    "        ranks[i] = tmp[0][0]\n",
    "        print(\"Rank: \"+str(ranks[i]))\n",
    "        if np.argsort(dist[i])[0] in nn:\n",
    "            nn[np.argsort(dist[i])[0]] += 1\n",
    "        else:\n",
    "            nn[np.argsort(dist[i])[0]] = 1 \n",
    "    print(nn)\n",
    "    print(ranks)  \n",
    "    mean += ranks.mean()\n",
    "    std +=ranks.std()\n",
    "    print(\"Mean: \"+str(mean/(act+1))+\" -- Std = \"+str(std/(act+1)))\n",
    "mean = mean/4.\n",
    "std = std/4.\n",
    "print(\"Final Mean: \"+str(mean)+\" -- Final Std = \"+str(std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SSA(object):\n",
    "    \n",
    "    __supported_types = (pd.Series, np.ndarray, list)\n",
    "    \n",
    "    def __init__(self, tseries, L, save_mem=True):\n",
    "        \"\"\"\n",
    "        Decomposes the given time series with a singular-spectrum analysis. Assumes the values of the time series are\n",
    "        recorded at equal intervals.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        tseries : The original time series, in the form of a Pandas Series, NumPy array or list. \n",
    "        L : The window length. Must be an integer 2 <= L <= N/2, where N is the length of the time series.\n",
    "        save_mem : Conserve memory by not retaining the elementary matrices. Recommended for long time series with\n",
    "            thousands of values. Defaults to True.\n",
    "        \n",
    "        Note: Even if an NumPy array or list is used for the initial time series, all time series returned will be\n",
    "        in the form of a Pandas Series or DataFrame object.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Tedious type-checking for the initial time series\n",
    "        if not isinstance(tseries, self.__supported_types):\n",
    "            raise TypeError(\"Unsupported time series object. Try Pandas Series, NumPy array or list.\")\n",
    "        \n",
    "        # Checks to save us from ourselves\n",
    "        self.N = len(tseries)\n",
    "        if not 2 <= L <= self.N/2:\n",
    "            raise ValueError(\"The window length must be in the interval [2, N/2].\")\n",
    "        \n",
    "        self.L = L\n",
    "        self.orig_TS = pd.Series(tseries)\n",
    "        self.K = self.N - self.L + 1\n",
    "        \n",
    "        # Embed the time series in a trajectory matrix\n",
    "        self.X = np.array([self.orig_TS.values[i:L+i] for i in range(0, self.K)]).T\n",
    "        \n",
    "        # Decompose the trajectory matrix\n",
    "        self.U, self.Sigma, VT = np.linalg.svd(self.X)\n",
    "        self.d = np.linalg.matrix_rank(self.X)\n",
    "        \n",
    "        self.TS_comps = np.zeros((self.N, self.d))\n",
    "        \n",
    "        if not save_mem:\n",
    "            # Construct and save all the elementary matrices\n",
    "            self.X_elem = np.array([ self.Sigma[i]*np.outer(self.U[:,i], VT[i,:]) for i in range(self.d) ])\n",
    "\n",
    "            # Diagonally average the elementary matrices, store them as columns in array.           \n",
    "            for i in range(self.d):\n",
    "                X_rev = self.X_elem[i, ::-1]\n",
    "                self.TS_comps[:,i] = [X_rev.diagonal(j).mean() for j in range(-X_rev.shape[0]+1, X_rev.shape[1])]\n",
    "            \n",
    "            self.V = VT.T\n",
    "        else:\n",
    "            # Reconstruct the elementary matrices without storing them\n",
    "            for i in range(self.d):\n",
    "                X_elem = self.Sigma[i]*np.outer(self.U[:,i], VT[i,:])\n",
    "                X_rev = X_elem[::-1]\n",
    "                self.TS_comps[:,i] = [X_rev.diagonal(j).mean() for j in range(-X_rev.shape[0]+1, X_rev.shape[1])]\n",
    "            \n",
    "            self.X_elem = \"Re-run with save_mem=False to retain the elementary matrices.\"\n",
    "            \n",
    "            # The V array may also be very large under these circumstances, so we won't keep it.\n",
    "            self.V = \"Re-run with save_mem=False to retain the V matrix.\"\n",
    "        \n",
    "        # Calculate the w-correlation matrix.\n",
    "        self.calc_wcorr()\n",
    "            \n",
    "    def components_to_df(self, n=0):\n",
    "        \"\"\"\n",
    "        Returns all the time series components in a single Pandas DataFrame object.\n",
    "        \"\"\"\n",
    "        if n > 0:\n",
    "            n = min(n, self.d)\n",
    "        else:\n",
    "            n = self.d\n",
    "        \n",
    "        # Create list of columns - call them F0, F1, F2, ...\n",
    "        cols = [\"F{}\".format(i) for i in range(n)]\n",
    "        return pd.DataFrame(self.TS_comps[:, :n], columns=cols, index=self.orig_TS.index)\n",
    "            \n",
    "    \n",
    "    def reconstruct(self, indices):\n",
    "        \"\"\"\n",
    "        Reconstructs the time series from its elementary components, using the given indices. Returns a Pandas Series\n",
    "        object with the reconstructed time series.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        indices: An integer, list of integers or slice(n,m) object, representing the elementary components to sum.\n",
    "        \"\"\"\n",
    "        if isinstance(indices, int): indices = [indices]\n",
    "        \n",
    "        ts_vals = self.TS_comps[:,indices].sum(axis=1)\n",
    "        return pd.Series(ts_vals, index=self.orig_TS.index)\n",
    "    \n",
    "    def calc_wcorr(self):\n",
    "        \"\"\"\n",
    "        Calculates the w-correlation matrix for the time series.\n",
    "        \"\"\"\n",
    "             \n",
    "        # Calculate the weights\n",
    "        w = np.array(list(np.arange(self.L)+1) + [self.L]*(self.K-self.L-1) + list(np.arange(self.L)+1)[::-1])\n",
    "        \n",
    "        def w_inner(F_i, F_j):\n",
    "            return w.dot(F_i*F_j)\n",
    "        \n",
    "        # Calculated weighted norms, ||F_i||_w, then invert.\n",
    "        F_wnorms = np.array([w_inner(self.TS_comps[:,i], self.TS_comps[:,i]) for i in range(self.d)])\n",
    "        F_wnorms = F_wnorms**-0.5\n",
    "        \n",
    "        # Calculate Wcorr.\n",
    "        self.Wcorr = np.identity(self.d)\n",
    "        for i in range(self.d):\n",
    "            for j in range(i+1,self.d):\n",
    "                self.Wcorr[i,j] = abs(w_inner(self.TS_comps[:,i], self.TS_comps[:,j]) * F_wnorms[i] * F_wnorms[j])\n",
    "                self.Wcorr[j,i] = self.Wcorr[i,j]\n",
    "    \n",
    "    def plot_wcorr(self, min=None, max=None):\n",
    "        \"\"\"\n",
    "        Plots the w-correlation matrix for the decomposed time series.\n",
    "        \"\"\"\n",
    "        if min is None:\n",
    "            min = 0\n",
    "        if max is None:\n",
    "            max = self.d\n",
    "        \n",
    "        if self.Wcorr is None:\n",
    "            self.calc_wcorr()\n",
    "        \n",
    "        ax = plt.imshow(self.Wcorr,interpolation = 'none')\n",
    "        plt.xlabel(r\"$\\tilde{F}_i$\")\n",
    "        plt.ylabel(r\"$\\tilde{F}_j$\")\n",
    "        plt.colorbar(ax.colorbar, fraction=0.045)\n",
    "        ax.colorbar.set_label(\"$W_{i,j}$\")\n",
    "        plt.clim(0,1)\n",
    "        \n",
    "        # For plotting purposes:\n",
    "        if max == self.d:\n",
    "            max_rnge = self.d-1\n",
    "        else:\n",
    "            max_rnge = max\n",
    "        \n",
    "        plt.xlim(min-0.5, max_rnge+0.5)\n",
    "        plt.ylim(max_rnge+0.5, min-0.5)\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test \n",
      "\n",
      "Now: 99.24%"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "window = 10 # SSA window == number of components\n",
    "ssa_test_data = test_data.copy()\n",
    "ssa_test_0 = []\n",
    "ssa_test_1 = []\n",
    "\n",
    "print(\"\\nTest \\n\")\n",
    "for i in range(len(ssa_test_data)):\n",
    "    ssa_test_0.append(SSA(ssa_test_data[i,0,:,0], window))\n",
    "    ssa_test_1.append(SSA(ssa_test_data[i,1,:,0], window))\n",
    "    if(i%100==1):\n",
    "        sys.stdout.write(\"\\rNow: \"+str(np.round(i*100/len(ssa_test_data), 2))+\"%\")\n",
    "        sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With 1 components:\n"
     ]
    }
   ],
   "source": [
    "tX = test_data.copy()\n",
    "\n",
    "num_comps = 1\n",
    "print(\"With \"+str(num_comps)+\" components:\")\n",
    " \n",
    "for i in range(len(tX)):\n",
    "    tX[i,0,:,0] = ssa_test_0[i].reconstruct(list(range(0,num_comps)))\n",
    "    tX[i,1,:,0] = ssa_test_1[i].reconstruct(list(range(0,num_comps)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(242, 2, 128, 1)\n",
      "Test User: 4\n",
      "j: 23\n",
      "Rank: 2.0\n",
      "Test User: 9\n",
      "j: 23\n",
      "Rank: 8.0\n",
      "Test User: 11\n",
      "j: 23\n",
      "Rank: 16.0\n",
      "Test User: 21\n",
      "j: 23\n",
      "Rank: 12.0\n",
      "{2: 2, 19: 2}\n",
      "[ 2.  8. 16. 12.]\n",
      "Mean: 9.5 -- Std = 5.172040216394301\n",
      "Test User: 4\n",
      "j: 23\n",
      "Rank: 4.0\n",
      "Test User: 9\n",
      "j: 23\n",
      "Rank: 15.0\n",
      "Test User: 11\n",
      "j: 23\n",
      "Rank: 6.0\n",
      "Test User: 21\n",
      "j: 23\n",
      "Rank: 18.0\n",
      "{2: 2, 19: 2, 10: 2, 6: 2}\n",
      "[ 4. 15.  6. 18.]\n",
      "Mean: 10.125 -- Std = 5.530824855544587\n",
      "Test User: 4\n",
      "j: 23\n",
      "Rank: 8.0\n",
      "Test User: 9\n",
      "j: 23\n",
      "Rank: 14.0\n",
      "Test User: 11\n",
      "j: 23\n",
      "Rank: 17.0\n",
      "Test User: 21\n",
      "j: 23\n",
      "Rank: 6.0\n",
      "{22: 3, 2: 2, 19: 2, 10: 2, 6: 3}\n",
      "[ 8. 14. 17.  6.]\n",
      "Mean: 10.5 -- Std = 5.166236516137962\n",
      "Test User: 4\n",
      "j: 23\n",
      "Rank: 4.0\n",
      "Test User: 9\n",
      "j: 23\n",
      "Rank: 17.0\n",
      "Test User: 11\n",
      "j: 23\n",
      "Rank: 6.0\n",
      "Test User: 21\n",
      "j: 23\n",
      "Rank: 0.0\n",
      "{2: 2, 19: 2, 6: 3, 22: 3, 7: 1, 10: 2, 21: 3}\n",
      "[ 4. 17.  6.  0.]\n",
      "Mean: 9.5625 -- Std = 5.449627783147741\n",
      "Final Mean: 9.5625 -- Final Std = 5.449627783147741\n"
     ]
    }
   ],
   "source": [
    "dist = np.zeros((4,24))\n",
    "ranks = np.zeros(4)\n",
    "kwd = KnnDtw(n_neighbors=10, subsample_step=2)\n",
    "nn = {} \n",
    "mean = 0\n",
    "std = 0\n",
    "dd = (w//s)*4\n",
    "\n",
    "## Labels\n",
    "rdu_act_X = act_train_labels[::dd]\n",
    "rdu_act_tX = act_test_labels[::dd]\n",
    "rdu_id_X = id_train_labels[::dd]\n",
    "rdu_id_tX = id_test_labels[::dd]\n",
    "\n",
    "# Test Data: after Transformation\n",
    "rdu_tX10 = tX[::dd]\n",
    "print(rdu_tX10.shape)\n",
    "\n",
    "\n",
    "# Training and Test Data: Raw\n",
    "rdu_Xraw = Xraw[::dd]\n",
    "rdu_tXraw = tXraw[::dd]\n",
    "rdu_all_Xraw = np.append(rdu_Xraw, rdu_tXraw, axis=0)\n",
    "rdu_all_act = np.append(rdu_act_X,rdu_act_tX,axis=0)\n",
    "rdu_all_id = np.append(rdu_id_X,rdu_id_tX,axis=0)\n",
    "\n",
    "\n",
    "import sys\n",
    "# just considering one activity, like walking. We want to compare data acrros users not activites. \n",
    "for act in range(4):\n",
    "    dic = {}\n",
    "    for i in range(len(test_ids)):\n",
    "        print(\"Test User: \"+str(test_ids[i]))\n",
    "        # Transformed Data\n",
    "        Xhi = rdu_tX10[np.logical_and(rdu_id_tX[:,test_ids[i]]==1., rdu_act_tX[:,act]==1.)]\n",
    "        for j in range(24):\n",
    "            # Raw Data\n",
    "            Xj = rdu_all_Xraw[np.logical_and(rdu_all_id[:,j]==1., rdu_all_act[:,act]==1.)]\n",
    "            mat = kwd._dist_matrix(Xhi[:,0,:,0], Xj[:,0,:,0])\n",
    "            s0 = (mat.sum())/(len(Xhi)*len(Xj))\n",
    "            \n",
    "            mat = kwd._dist_matrix(Xhi[:,1,:,0], Xj[:,1,:,0])\n",
    "            s1 = (mat.sum())/(len(Xhi)*len(Xj))\n",
    "            dist[i,j] = (s0+s1)/2.\n",
    "            \n",
    "            sys.stdout.write(\"\\rj: \"+str(j))\n",
    "            sys.stdout.flush()\n",
    "\n",
    "        print(\"\")    \n",
    "        tmp = np.argsort(dist[i])\n",
    "        tmp = np.where(tmp == test_ids[i])\n",
    "        ranks[i] = tmp[0][0]\n",
    "        print(\"Rank: \"+str(ranks[i]))\n",
    "        if np.argsort(dist[i])[0] in nn:\n",
    "            nn[np.argsort(dist[i])[0]] += 1\n",
    "        else:\n",
    "            nn[np.argsort(dist[i])[0]] = 1 \n",
    "    print(nn)\n",
    "    print(ranks)  \n",
    "    mean += ranks.mean()\n",
    "    std +=ranks.std()\n",
    "    print(\"Mean: \"+str(mean/(act+1))+\" -- Std = \"+str(std/(act+1)))\n",
    "mean = mean/4.\n",
    "std = std/4.\n",
    "print(\"Final Mean: \"+str(mean)+\" -- Final Std = \"+str(std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With 2 components:\n"
     ]
    }
   ],
   "source": [
    "tX = test_data.copy()\n",
    "\n",
    "num_comps = 2\n",
    "print(\"With \"+str(num_comps)+\" components:\")\n",
    " \n",
    "for i in range(len(tX)):\n",
    "    tX[i,0,:,0] = ssa_test_0[i].reconstruct(list(range(0,num_comps)))\n",
    "    tX[i,1,:,0] = ssa_test_1[i].reconstruct(list(range(0,num_comps)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(242, 2, 128, 1)\n",
      "Test User: 4\n",
      "j: 23\n",
      "Rank: 2.0\n",
      "Test User: 9\n",
      "j: 23\n",
      "Rank: 5.0\n",
      "Test User: 11\n",
      "j: 23\n",
      "Rank: 15.0\n",
      "Test User: 21\n",
      "j: 23\n",
      "Rank: 10.0\n",
      "{2: 1, 19: 3}\n",
      "[ 2.  5. 15. 10.]\n",
      "Mean: 8.0 -- Std = 4.949747468305833\n",
      "Test User: 4\n",
      "j: 23\n",
      "Rank: 0.0\n",
      "Test User: 9\n",
      "j: 23\n",
      "Rank: 13.0\n",
      "Test User: 11\n",
      "j: 23\n",
      "Rank: 1.0\n",
      "Test User: 21\n",
      "j: 23\n",
      "Rank: 13.0\n",
      "{2: 1, 19: 3, 4: 1, 10: 2, 22: 1}\n",
      "[ 0. 13.  1. 13.]\n",
      "Mean: 7.375 -- Std = 5.6048697405401455\n",
      "Test User: 4\n",
      "j: 23\n",
      "Rank: 1.0\n",
      "Test User: 9\n",
      "j: 23\n",
      "Rank: 13.0\n",
      "Test User: 11\n",
      "j: 23\n",
      "Rank: 13.0\n",
      "Test User: 21\n",
      "j: 23\n",
      "Rank: 3.0\n",
      "{2: 1, 19: 3, 4: 1, 22: 4, 20: 1, 10: 2}\n",
      "[ 1. 13. 13.  3.]\n",
      "Mean: 7.416666666666667 -- Std = 5.585002578095001\n",
      "Test User: 4\n",
      "j: 23\n",
      "Rank: 0.0\n",
      "Test User: 9\n",
      "j: 23\n",
      "Rank: 14.0\n",
      "Test User: 11\n",
      "j: 23\n",
      "Rank: 7.0\n",
      "Test User: 21\n",
      "j: 23\n",
      "Rank: 0.0\n",
      "{2: 1, 19: 3, 4: 3, 21: 2, 22: 4, 20: 1, 10: 2}\n",
      "[ 0. 14.  7.  0.]\n",
      "Mean: 6.875 -- Std = 5.639775279351738\n",
      "Final Mean: 6.875 -- Final Std = 5.639775279351738\n"
     ]
    }
   ],
   "source": [
    "dist = np.zeros((4,24))\n",
    "ranks = np.zeros(4)\n",
    "kwd = KnnDtw(n_neighbors=10, subsample_step=2)\n",
    "nn = {} \n",
    "mean = 0\n",
    "std = 0\n",
    "dd = (w//s)*4\n",
    "\n",
    "## Labels\n",
    "rdu_act_X = act_train_labels[::dd]\n",
    "rdu_act_tX = act_test_labels[::dd]\n",
    "rdu_id_X = id_train_labels[::dd]\n",
    "rdu_id_tX = id_test_labels[::dd]\n",
    "\n",
    "# Test Data: after Transformation\n",
    "rdu_tX10 = tX[::dd]\n",
    "print(rdu_tX10.shape)\n",
    "\n",
    "\n",
    "# Training and Test Data: Raw\n",
    "rdu_Xraw = Xraw[::dd]\n",
    "rdu_tXraw = tXraw[::dd]\n",
    "rdu_all_Xraw = np.append(rdu_Xraw, rdu_tXraw, axis=0)\n",
    "rdu_all_act = np.append(rdu_act_X,rdu_act_tX,axis=0)\n",
    "rdu_all_id = np.append(rdu_id_X,rdu_id_tX,axis=0)\n",
    "\n",
    "\n",
    "import sys\n",
    "# just considering one activity, like walking. We want to compare data acrros users not activites. \n",
    "for act in range(4):\n",
    "    dic = {}\n",
    "    for i in range(len(test_ids)):\n",
    "        print(\"Test User: \"+str(test_ids[i]))\n",
    "        # Transformed Data\n",
    "        Xhi = rdu_tX10[np.logical_and(rdu_id_tX[:,test_ids[i]]==1., rdu_act_tX[:,act]==1.)]\n",
    "        for j in range(24):\n",
    "            # Raw Data\n",
    "            Xj = rdu_all_Xraw[np.logical_and(rdu_all_id[:,j]==1., rdu_all_act[:,act]==1.)]\n",
    "            mat = kwd._dist_matrix(Xhi[:,0,:,0], Xj[:,0,:,0])\n",
    "            s0 = (mat.sum())/(len(Xhi)*len(Xj))\n",
    "            \n",
    "            mat = kwd._dist_matrix(Xhi[:,1,:,0], Xj[:,1,:,0])\n",
    "            s1 = (mat.sum())/(len(Xhi)*len(Xj))\n",
    "            dist[i,j] = (s0+s1)/2.\n",
    "            \n",
    "            sys.stdout.write(\"\\rj: \"+str(j))\n",
    "            sys.stdout.flush()\n",
    "\n",
    "        print(\"\")    \n",
    "        tmp = np.argsort(dist[i])\n",
    "        tmp = np.where(tmp == test_ids[i])\n",
    "        ranks[i] = tmp[0][0]\n",
    "        print(\"Rank: \"+str(ranks[i]))\n",
    "        if np.argsort(dist[i])[0] in nn:\n",
    "            nn[np.argsort(dist[i])[0]] += 1\n",
    "        else:\n",
    "            nn[np.argsort(dist[i])[0]] = 1 \n",
    "    print(nn)\n",
    "    print(ranks)  \n",
    "    mean += ranks.mean()\n",
    "    std +=ranks.std()\n",
    "    print(\"Mean: \"+str(mean/(act+1))+\" -- Std = \"+str(std/(act+1)))\n",
    "mean = mean/4.\n",
    "std = std/4.\n",
    "print(\"Final Mean: \"+str(mean)+\" -- Final Std = \"+str(std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
