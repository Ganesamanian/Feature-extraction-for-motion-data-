{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "import keras \n",
    "import keras.backend as K\n",
    "from scipy.signal import resample\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
    "from collections import Counter\n",
    "\n",
    "from keras import regularizers\n",
    "from keras.models import Sequential, Model, load_model, model_from_json \n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Input, Dense, Flatten, Reshape, Concatenate,  Dropout \n",
    "from keras.layers import Conv2D, MaxPooling2D, UpSampling2D, Conv2DTranspose\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "\n",
    "def get_class_weights(y):\n",
    "    counter = Counter(y)\n",
    "    majority = max(counter.values())\n",
    "    return  {cls: float(majority/count) for cls, count in counter.items()}\n",
    "\n",
    "\n",
    "\n",
    "class Estimator:\n",
    "    l2p = 0.001\n",
    "    @staticmethod\n",
    "    def early_layers(inp, fm = (1,3), hid_act_func=\"relu\"):\n",
    "        # Start\n",
    "        x = Conv2D(32, fm, padding=\"same\", kernel_regularizer=regularizers.l2(Estimator.l2p), activation=hid_act_func)(inp)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = MaxPooling2D(pool_size=(1, 2))(x)\n",
    "        x = Dropout(0.25)(x)\n",
    "        \n",
    "        # 1\n",
    "        x = Conv2D(32, fm, padding=\"same\", kernel_regularizer=regularizers.l2(Estimator.l2p), activation=hid_act_func)(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = MaxPooling2D(pool_size=(1, 2))(x)\n",
    "        x = Dropout(0.25)(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    @staticmethod\n",
    "    def late_layers(inp, num_classes, fm = (1,3), act_func=\"softmax\", hid_act_func=\"relu\", b_name=\"Identifier\"):\n",
    "        # 2\n",
    "        x = Conv2D(32, fm, padding=\"same\", kernel_regularizer=regularizers.l2(Estimator.l2p), activation=hid_act_func)(inp)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = MaxPooling2D(pool_size=(1, 2))(x)\n",
    "        x = Dropout(0.25)(x)\n",
    "        \n",
    "        \n",
    "        # End\n",
    "        x = Flatten()(x)\n",
    "        x = Dense(128, kernel_regularizer=regularizers.l2(Estimator.l2p), activation=hid_act_func)(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.5)(x)\n",
    "        x = Dense(32, kernel_regularizer=regularizers.l2(Estimator.l2p), activation=hid_act_func)(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.5)(x)\n",
    "        x = Dense(num_classes, activation=act_func, name = b_name)(x)\n",
    "\n",
    "        return x\n",
    "   \n",
    "    @staticmethod\n",
    "    def build(height, width, num_classes, name, fm = (1,3), act_func=\"softmax\",hid_act_func=\"relu\"):\n",
    "        inp = Input(shape=(height, width, 1))\n",
    "        early = Estimator.early_layers(inp, fm, hid_act_func=hid_act_func)\n",
    "        late  = Estimator.late_layers(early, num_classes, fm, act_func=act_func, hid_act_func=hid_act_func)\n",
    "        model = Model(inputs=inp, outputs=late ,name=name)\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.plotting import autocorrelation_plot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def get_ds_infos():\n",
    "    \"\"\"\n",
    "    Read the file includes data subject information.\n",
    "    \n",
    "    Data Columns:\n",
    "    0: code [1-24]\n",
    "    1: weight [kg]\n",
    "    2: height [cm]\n",
    "    3: age [years]\n",
    "    4: gender [0:Female, 1:Male]\n",
    "    \n",
    "    Returns:\n",
    "        A pandas DataFrame that contains inforamtion about data subjects' attributes \n",
    "    \"\"\" \n",
    "\n",
    "    dss = pd.read_csv(\"data_subjects_info.csv\")\n",
    "    print(\"[INFO] -- Data subjects' information is imported.\")\n",
    "    \n",
    "    return dss\n",
    "\n",
    "def set_data_types(data_types=[\"userAcceleration\"]):\n",
    "    \"\"\"\n",
    "    Select the sensors and the mode to shape the final dataset.\n",
    "    \n",
    "    Args:\n",
    "        data_types: A list of sensor data type from this list: [attitude, gravity, rotationRate, userAcceleration] \n",
    "\n",
    "    Returns:\n",
    "        It returns a list of columns to use for creating time-series from files.\n",
    "    \"\"\"\n",
    "    dt_list = []\n",
    "    for t in data_types:\n",
    "        if t != \"attitude\":\n",
    "            dt_list.append([t+\".x\",t+\".y\",t+\".z\"])\n",
    "        else:\n",
    "            dt_list.append([t+\".roll\", t+\".pitch\", t+\".yaw\"])\n",
    "\n",
    "    return dt_list\n",
    "\n",
    "\n",
    "def creat_time_series(dt_list, act_labels, trial_codes, mode=\"mag\", labeled=True, combine_grav_acc=False):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        dt_list: A list of columns that shows the type of data we want.\n",
    "        act_labels: list of activites\n",
    "        trial_codes: list of trials\n",
    "        mode: It can be \"raw\" which means you want raw data\n",
    "        for every dimention of each data type,\n",
    "        [attitude(roll, pitch, yaw); gravity(x, y, z); rotationRate(x, y, z); userAcceleration(x,y,z)].\n",
    "        or it can be \"mag\" which means you only want the magnitude for each data type: (x^2+y^2+z^2)^(1/2)\n",
    "        labeled: True, if we want a labeld dataset. False, if we only want sensor values.\n",
    "        combine_grav_acc: True, means adding each axis of gravity to  corresponding axis of userAcceleration.\n",
    "    Returns: \n",
    "        It returns a time-series of sensor data.\n",
    "    \n",
    "    \"\"\"\n",
    "    num_data_cols = len(dt_list) if mode == \"mag\" else len(dt_list*3)\n",
    "\n",
    "    if labeled:\n",
    "        dataset = np.zeros((0,num_data_cols+7)) # \"7\" --> [act, code, weight, height, age, gender, trial] \n",
    "    else:\n",
    "        dataset = np.zeros((0,num_data_cols))\n",
    "        \n",
    "    ds_list = get_ds_infos()\n",
    "    \n",
    "    print(\"[INFO] -- Creating Time-Series\")\n",
    "    for sub_id in ds_list[\"code\"]:\n",
    "        for act_id, act in enumerate(act_labels):\n",
    "            for trial in trial_codes[act_id]:\n",
    "                fname = 'A_DeviceMotion_data/'+act+'_'+str(trial)+'/sub_'+str(int(sub_id))+'.csv'\n",
    "                raw_data = pd.read_csv(fname)\n",
    "                raw_data = raw_data.drop(['Unnamed: 0'], axis=1)\n",
    "                vals = np.zeros((len(raw_data), num_data_cols))\n",
    "                \n",
    "                if combine_grav_acc:\n",
    "                    raw_data[\"userAcceleration.x\"] = raw_data[\"userAcceleration.x\"].add(raw_data[\"gravity.x\"])\n",
    "                    raw_data[\"userAcceleration.y\"] = raw_data[\"userAcceleration.y\"].add(raw_data[\"gravity.y\"])\n",
    "                    raw_data[\"userAcceleration.z\"] = raw_data[\"userAcceleration.z\"].add(raw_data[\"gravity.z\"])\n",
    "                \n",
    "                for x_id, axes in enumerate(dt_list):\n",
    "                    if mode == \"mag\":\n",
    "                        vals[:,x_id] = (raw_data[axes]**2).sum(axis=1)**0.5        \n",
    "                    else:\n",
    "                        vals[:,x_id*3:(x_id+1)*3] = raw_data[axes].values\n",
    "                    vals = vals[:,:num_data_cols]\n",
    "                if labeled:\n",
    "                    lbls = np.array([[act_id,\n",
    "                            sub_id-1,\n",
    "                            ds_list[\"weight\"][sub_id-1],\n",
    "                            ds_list[\"height\"][sub_id-1],\n",
    "                            ds_list[\"age\"][sub_id-1],\n",
    "                            ds_list[\"gender\"][sub_id-1],\n",
    "                            trial          \n",
    "                           ]]*len(raw_data))\n",
    "                    vals = np.concatenate((vals, lbls), axis=1)\n",
    "                dataset = np.append(dataset,vals, axis=0)\n",
    "    cols = []\n",
    "    for axes in dt_list:\n",
    "        if mode == \"raw\":\n",
    "            cols += axes\n",
    "        else:\n",
    "            cols += [str(axes[0][:-2])]\n",
    "            \n",
    "    if labeled:\n",
    "        cols += [\"act\", \"id\", \"weight\", \"height\", \"age\", \"gender\", \"trial\"]\n",
    "    \n",
    "    dataset = pd.DataFrame(data=dataset, columns=cols)\n",
    "    return dataset\n",
    "#________________________________\n",
    "#________________________________\n",
    "\n",
    "def ts_to_secs(dataset, w, s, standardize = False, **options):\n",
    "    \n",
    "    data = dataset[dataset.columns[:-7]].values    \n",
    "    act_labels = dataset[\"act\"].values\n",
    "    id_labels = dataset[\"id\"].values\n",
    "    trial_labels = dataset[\"trial\"].values\n",
    "\n",
    "    mean = 0\n",
    "    std = 1\n",
    "    if standardize:\n",
    "        ## Standardize each sensor’s data to have a zero mean and unity standard deviation.\n",
    "        ## As usual, we normalize test dataset by training dataset's parameters \n",
    "        if options:\n",
    "            mean = options.get(\"mean\")\n",
    "            std = options.get(\"std\")\n",
    "            print(\"[INFO] -- Test Data has been standardized\")\n",
    "        else:\n",
    "            mean = data.mean(axis=0)\n",
    "            std = data.std(axis=0)\n",
    "            print(\"[INFO] -- Training Data has been standardized: the mean is = \"+str(mean)+\" ; and the std is = \"+str(std))            \n",
    "\n",
    "        data -= mean\n",
    "        data /= std\n",
    "    else:\n",
    "        print(\"[INFO] -- Without Standardization.....\")\n",
    "\n",
    "    ## We want the Rows of matrices show each Feature and the Columns show time points.\n",
    "    data = data.T\n",
    "\n",
    "    m = data.shape[0]   # Data Dimension \n",
    "    ttp = data.shape[1] # Total Time Points\n",
    "    number_of_secs = int(round(((ttp - w)/s)))\n",
    "\n",
    "    ##  Create a 3D matrix for Storing Sections  \n",
    "    secs_data = np.zeros((number_of_secs , m , w ))\n",
    "    act_secs_labels = np.zeros(number_of_secs)\n",
    "    id_secs_labels = np.zeros(number_of_secs)\n",
    "\n",
    "    k=0\n",
    "    for i in range(0 , ttp-w, s):\n",
    "        j = i // s\n",
    "        if j >= number_of_secs:\n",
    "            break\n",
    "        if id_labels[i] != id_labels[i+w-1]: \n",
    "            continue\n",
    "        if act_labels[i] != act_labels[i+w-1]: \n",
    "            continue\n",
    "        if trial_labels[i] != trial_labels[i+w-1]:\n",
    "            continue\n",
    "            \n",
    "        secs_data[k] = data[:, i:i+w]\n",
    "        act_secs_labels[k] = act_labels[i].astype(int)\n",
    "        id_secs_labels[k] = id_labels[i].astype(int)\n",
    "        k = k+1\n",
    "        \n",
    "    secs_data = secs_data[0:k]\n",
    "    act_secs_labels = act_secs_labels[0:k]\n",
    "    id_secs_labels = id_secs_labels[0:k]\n",
    "    return secs_data, act_secs_labels, id_secs_labels, mean, std\n",
    "##________________________________________________________________\n",
    "\n",
    "\n",
    "ACT_LABELS = [\"dws\",\"ups\", \"wlk\", \"jog\", \"std\", \"sit\"]\n",
    "TRIAL_CODES = {\n",
    "    ACT_LABELS[0]:[1,2,11],\n",
    "    ACT_LABELS[1]:[3,4,12],\n",
    "    ACT_LABELS[2]:[7,8,15],\n",
    "    ACT_LABELS[3]:[9,16],\n",
    "    ACT_LABELS[4]:[6,14],\n",
    "    ACT_LABELS[5]:[5,13],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SSA(object):\n",
    "    \n",
    "    __supported_types = (pd.Series, np.ndarray, list)\n",
    "    \n",
    "    def __init__(self, tseries, L, save_mem=True):\n",
    "        \"\"\"\n",
    "        Decomposes the given time series with a singular-spectrum analysis. Assumes the values of the time series are\n",
    "        recorded at equal intervals.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        tseries : The original time series, in the form of a Pandas Series, NumPy array or list. \n",
    "        L : The window length. Must be an integer 2 <= L <= N/2, where N is the length of the time series.\n",
    "        save_mem : Conserve memory by not retaining the elementary matrices. Recommended for long time series with\n",
    "            thousands of values. Defaults to True.\n",
    "        \n",
    "        Note: Even if an NumPy array or list is used for the initial time series, all time series returned will be\n",
    "        in the form of a Pandas Series or DataFrame object.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Tedious type-checking for the initial time series\n",
    "        if not isinstance(tseries, self.__supported_types):\n",
    "            raise TypeError(\"Unsupported time series object. Try Pandas Series, NumPy array or list.\")\n",
    "        \n",
    "        # Checks to save us from ourselves\n",
    "        self.N = len(tseries)\n",
    "        if not 2 <= L <= self.N/2:\n",
    "            raise ValueError(\"The window length must be in the interval [2, N/2].\")\n",
    "        \n",
    "        self.L = L\n",
    "        self.orig_TS = pd.Series(tseries)\n",
    "        self.K = self.N - self.L + 1\n",
    "        \n",
    "        # Embed the time series in a trajectory matrix\n",
    "        self.X = np.array([self.orig_TS.values[i:L+i] for i in range(0, self.K)]).T\n",
    "        \n",
    "        # Decompose the trajectory matrix\n",
    "        self.U, self.Sigma, VT = np.linalg.svd(self.X)\n",
    "        self.d = np.linalg.matrix_rank(self.X)\n",
    "        \n",
    "        self.TS_comps = np.zeros((self.N, self.d))\n",
    "        \n",
    "        if not save_mem:\n",
    "            # Construct and save all the elementary matrices\n",
    "            self.X_elem = np.array([ self.Sigma[i]*np.outer(self.U[:,i], VT[i,:]) for i in range(self.d) ])\n",
    "\n",
    "            # Diagonally average the elementary matrices, store them as columns in array.           \n",
    "            for i in range(self.d):\n",
    "                X_rev = self.X_elem[i, ::-1]\n",
    "                self.TS_comps[:,i] = [X_rev.diagonal(j).mean() for j in range(-X_rev.shape[0]+1, X_rev.shape[1])]\n",
    "            \n",
    "            self.V = VT.T\n",
    "        else:\n",
    "            # Reconstruct the elementary matrices without storing them\n",
    "            for i in range(self.d):\n",
    "                X_elem = self.Sigma[i]*np.outer(self.U[:,i], VT[i,:])\n",
    "                X_rev = X_elem[::-1]\n",
    "                self.TS_comps[:,i] = [X_rev.diagonal(j).mean() for j in range(-X_rev.shape[0]+1, X_rev.shape[1])]\n",
    "            \n",
    "            self.X_elem = \"Re-run with save_mem=False to retain the elementary matrices.\"\n",
    "            \n",
    "            # The V array may also be very large under these circumstances, so we won't keep it.\n",
    "            self.V = \"Re-run with save_mem=False to retain the V matrix.\"\n",
    "        \n",
    "        # Calculate the w-correlation matrix.\n",
    "        self.calc_wcorr()\n",
    "            \n",
    "    def components_to_df(self, n=0):\n",
    "        \"\"\"\n",
    "        Returns all the time series components in a single Pandas DataFrame object.\n",
    "        \"\"\"\n",
    "        if n > 0:\n",
    "            n = min(n, self.d)\n",
    "        else:\n",
    "            n = self.d\n",
    "        \n",
    "        # Create list of columns - call them F0, F1, F2, ...\n",
    "        cols = [\"F{}\".format(i) for i in range(n)]\n",
    "        return pd.DataFrame(self.TS_comps[:, :n], columns=cols, index=self.orig_TS.index)\n",
    "            \n",
    "    \n",
    "    def reconstruct(self, indices):\n",
    "        \"\"\"\n",
    "        Reconstructs the time series from its elementary components, using the given indices. Returns a Pandas Series\n",
    "        object with the reconstructed time series.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        indices: An integer, list of integers or slice(n,m) object, representing the elementary components to sum.\n",
    "        \"\"\"\n",
    "        if isinstance(indices, int): indices = [indices]\n",
    "        \n",
    "        ts_vals = self.TS_comps[:,indices].sum(axis=1)\n",
    "        return pd.Series(ts_vals, index=self.orig_TS.index)\n",
    "    \n",
    "    def calc_wcorr(self):\n",
    "        \"\"\"\n",
    "        Calculates the w-correlation matrix for the time series.\n",
    "        \"\"\"\n",
    "             \n",
    "        # Calculate the weights\n",
    "        w = np.array(list(np.arange(self.L)+1) + [self.L]*(self.K-self.L-1) + list(np.arange(self.L)+1)[::-1])\n",
    "        \n",
    "        def w_inner(F_i, F_j):\n",
    "            return w.dot(F_i*F_j)\n",
    "        \n",
    "        # Calculated weighted norms, ||F_i||_w, then invert.\n",
    "        F_wnorms = np.array([w_inner(self.TS_comps[:,i], self.TS_comps[:,i]) for i in range(self.d)])\n",
    "        F_wnorms = F_wnorms**-0.5\n",
    "        \n",
    "        # Calculate Wcorr.\n",
    "        self.Wcorr = np.identity(self.d)\n",
    "        for i in range(self.d):\n",
    "            for j in range(i+1,self.d):\n",
    "                self.Wcorr[i,j] = abs(w_inner(self.TS_comps[:,i], self.TS_comps[:,j]) * F_wnorms[i] * F_wnorms[j])\n",
    "                self.Wcorr[j,i] = self.Wcorr[i,j]\n",
    "    \n",
    "    def plot_wcorr(self, min=None, max=None):\n",
    "        \"\"\"\n",
    "        Plots the w-correlation matrix for the decomposed time series.\n",
    "        \"\"\"\n",
    "        if min is None:\n",
    "            min = 0\n",
    "        if max is None:\n",
    "            max = self.d\n",
    "        \n",
    "        if self.Wcorr is None:\n",
    "            self.calc_wcorr()\n",
    "        \n",
    "        ax = plt.imshow(self.Wcorr,interpolation = 'none')\n",
    "        plt.xlabel(r\"$\\tilde{F}_i$\")\n",
    "        plt.ylabel(r\"$\\tilde{F}_j$\")\n",
    "        plt.colorbar(ax.colorbar, fraction=0.045)\n",
    "        ax.colorbar.set_label(\"$W_{i,j}$\")\n",
    "        plt.clim(0,1)\n",
    "        \n",
    "        # For plotting purposes:\n",
    "        if max == self.d:\n",
    "            max_rnge = self.d-1\n",
    "        else:\n",
    "            max_rnge = max\n",
    "        \n",
    "        plt.xlim(min-0.5, max_rnge+0.5)\n",
    "        plt.ylim(max_rnge+0.5, min-0.5)\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/a/45305384/5210098\n",
    "def f1_metric(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "\n",
    "        Only computes a batch-wise average of recall.\n",
    "\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "\n",
    "        Only computes a batch-wise average of precision.\n",
    "\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] -- Selected sensor data types: ['rotationRate', 'userAcceleration'] -- Mode: mag -- Grav+Acc: True\n",
      "[INFO] -- Selected activites: ['dws', 'ups', 'wlk', 'jog']\n",
      "[INFO] -- Data subjects' information is imported.\n",
      "[INFO] -- Creating Time-Series\n",
      "[INFO] -- Shape of time-Series dataset:(767660, 9)\n",
      "[INFO] -- Test Trials: [11, 12, 13, 14, 15, 16]\n",
      "[INFO] -- Shape of Train Time-Series :(621973, 9)\n",
      "[INFO] -- Shape of Test Time-Series :(145687, 9)\n",
      "___________________________________________________\n",
      "   rotationRate  userAcceleration  act   id  weight  height   age  gender  \\\n",
      "0      1.370498          1.195847  0.0  0.0   102.0   188.0  46.0     1.0   \n",
      "1      1.141648          1.196990  0.0  0.0   102.0   188.0  46.0     1.0   \n",
      "2      0.372530          1.117437  0.0  0.0   102.0   188.0  46.0     1.0   \n",
      "3      1.049628          1.088320  0.0  0.0   102.0   188.0  46.0     1.0   \n",
      "4      0.921229          1.390551  0.0  0.0   102.0   188.0  46.0     1.0   \n",
      "\n",
      "   trial  \n",
      "0    1.0  \n",
      "1    1.0  \n",
      "2    1.0  \n",
      "3    1.0  \n",
      "4    1.0  \n",
      "[INFO] -- Training Data has been standardized: the mean is = [2.20896278 1.19815844] ; and the std is = [1.42146386 0.70139403]\n",
      "[INFO] -- Test Data has been standardized\n",
      "[INFO] -- Training Sections: (60059, 2, 128)\n",
      "[INFO] -- Test Sections:  (13343, 2, 128)\n"
     ]
    }
   ],
   "source": [
    "## Here we set parameter to build labeld time-series from dataset of \"(A)DeviceMotion_data\"\n",
    "## attitude(roll, pitch, yaw); gravity(x, y, z); rotationRate(x, y, z); userAcceleration(x,y,z)\n",
    "results ={}\n",
    "sdt = [\"rotationRate\",\"userAcceleration\"]\n",
    "mode = \"mag\"\n",
    "cga = True # Add gravity to acceleration or not\n",
    "\n",
    "print(\"[INFO] -- Selected sensor data types: \"+str(sdt)+\" -- Mode: \"+str(mode)+\" -- Grav+Acc: \"+str(cga))    \n",
    "act_labels = ACT_LABELS [0:4]\n",
    "\n",
    "print(\"[INFO] -- Selected activites: \"+str(act_labels))    \n",
    "trial_codes = [TRIAL_CODES[act] for act in act_labels]\n",
    "dt_list = set_data_types(sdt)\n",
    "dataset = creat_time_series(dt_list, act_labels, trial_codes, mode=mode, labeled=True, combine_grav_acc = cga)\n",
    "print(\"[INFO] -- Shape of time-Series dataset:\"+str(dataset.shape))    \n",
    "\n",
    "\n",
    "#*****************\n",
    "TRAIN_TEST_TYPE = \"trial\" # \"subject\" or \"trial\"\n",
    "#*****************\n",
    "\n",
    "if TRAIN_TEST_TYPE == \"subject\":\n",
    "    test_ids = [4,9,11,21]\n",
    "    print(\"[INFO] -- Test IDs: \"+str(test_ids))\n",
    "    test_ts = dataset.loc[(dataset['id'].isin(test_ids))]\n",
    "    train_ts = dataset.loc[~(dataset['id'].isin(test_ids))]\n",
    "else:\n",
    "    test_trail = [11,12,13,14,15,16]  \n",
    "    print(\"[INFO] -- Test Trials: \"+str(test_trail))\n",
    "    test_ts = dataset.loc[(dataset['trial'].isin(test_trail))]\n",
    "    train_ts = dataset.loc[~(dataset['trial'].isin(test_trail))]\n",
    "\n",
    "print(\"[INFO] -- Shape of Train Time-Series :\"+str(train_ts.shape))\n",
    "print(\"[INFO] -- Shape of Test Time-Series :\"+str(test_ts.shape))\n",
    "\n",
    "# print(\"___________Train_VAL____________\")\n",
    "# val_trail = [11,12,13,14,15,16]\n",
    "# val_ts = train_ts.loc[(train_ts['trial'].isin(val_trail))]\n",
    "# train_ts = train_ts.loc[~(train_ts['trial'].isin(val_trail))]\n",
    "# print(\"[INFO] -- Training Time-Series :\"+str(train_ts.shape))\n",
    "# print(\"[INFO] -- Validation Time-Series :\"+str(val_ts.shape))\n",
    "\n",
    "print(\"___________________________________________________\")\n",
    "print(train_ts.head())\n",
    "\n",
    "## This Variable Defines the Size of Sliding Window\n",
    "## ( e.g. 100 means in each snapshot we just consider 100 consecutive observations of each sensor) \n",
    "w = 128 # 50 Equals to 1 second for MotionSense Dataset (it is on 50Hz samplig rate)\n",
    "## Here We Choose Step Size for Building Diffrent Snapshots from Time-Series Data\n",
    "## ( smaller step size will increase the amount of the instances and higher computational cost may be incurred )\n",
    "s = 10\n",
    "train_data, act_train, id_train, train_mean, train_std = ts_to_secs(train_ts.copy(),\n",
    "                                                                   w,\n",
    "                                                                   s,\n",
    "                                                                   standardize = True)\n",
    "\n",
    "s = 10\n",
    "# val_data, act_val, id_val, val_mean, val_std = ts_to_secs(val_ts.copy(),\n",
    "#                                                           w,\n",
    "#                                                           s,\n",
    "#                                                           standardize = True,\n",
    "#                                                           mean = train_mean, \n",
    "#                                                           std = train_std)\n",
    "\n",
    "s = 10\n",
    "test_data, act_test, id_test, test_mean, test_std = ts_to_secs(test_ts.copy(),\n",
    "                                                              w,\n",
    "                                                              s,\n",
    "                                                              standardize = True,\n",
    "                                                              mean = train_mean, \n",
    "                                                              std = train_std)\n",
    "\n",
    "print(\"[INFO] -- Training Sections: \"+str(train_data.shape))\n",
    "#print(\"[INFO] -- Validation Sections: \"+str(val_data.shape))\n",
    "print(\"[INFO] -- Test Sections:  \"+str(test_data.shape))\n",
    "\n",
    "\n",
    "id_train_labels = to_categorical(id_train)\n",
    "#id_val_labels = to_categorical(id_val)\n",
    "id_test_labels = to_categorical(id_test)\n",
    "\n",
    "act_train_labels = to_categorical(act_train)\n",
    "#act_val_labels = to_categorical(act_val)\n",
    "act_test_labels = to_categorical(act_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, act_train_labels, id_train_labels = shuffle(train_data, act_train_labels, id_train_labels, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('[INFO] -- Shape of Training Sections:', (60059, 2, 128, 1))\n",
      "('[INFO] -- Shape of Training Sections:', (13343, 2, 128, 1))\n"
     ]
    }
   ],
   "source": [
    "## Here we add an extra dimension to the datasets just to be ready for using with Convolution2D\n",
    "train_data = np.expand_dims(train_data,axis=3)\n",
    "print(\"[INFO] -- Shape of Training Sections:\", train_data.shape)\n",
    "# val_data = np.expand_dims(val_data,axis=3)\n",
    "# print(\"[INFO] -- Validation Sections:\"+str(val_data.shape))\n",
    "test_data = np.expand_dims(test_data,axis=3)\n",
    "print(\"[INFO] -- Shape of Training Sections:\", test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train \n",
      "\n",
      "Now: 99%\n",
      " Test \n",
      "\n",
      "Now: 99%"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "window = 10 # SSA window == number of components\n",
    "ssa_train_data = train_data.copy()\n",
    "# ssa_val_data = val_data.copy()\n",
    "ssa_test_data = test_data.copy()\n",
    "ssa_train_0 = []\n",
    "ssa_train_1 = []\n",
    "# ssa_val_0 = []\n",
    "# ssa_val_1 = []\n",
    "ssa_test_0 = []\n",
    "ssa_test_1 = []\n",
    "\n",
    "print(\"\\n Train \\n\")\n",
    "for i in range(len(ssa_train_data)):\n",
    "    ssa_train_0.append(SSA(ssa_train_data[i,0,:,0], window))\n",
    "    ssa_train_1.append(SSA(ssa_train_data[i,1,:,0], window))\n",
    "    if(i%100==1):\n",
    "        sys.stdout.write(\"\\rNow: \"+str(np.round(i*100/len(ssa_train_data), 2))+\"%\")\n",
    "        sys.stdout.flush()\n",
    "\n",
    "# print(\"\\n Val \\n\")\n",
    "# for i in range(len(ssa_val_data)):\n",
    "#     ssa_val_0.append(SSA(ssa_val_data[i,0,:,0], window))\n",
    "#     ssa_val_1.append(SSA(ssa_val_data[i,1,:,0], window))\n",
    "#     if(i%100==1):\n",
    "#         sys.stdout.write(\"\\rNow: \"+str(np.round(i*100/len(ssa_val_data), 2))+\"%\")\n",
    "#         sys.stdout.flush()        \n",
    "\n",
    "print(\"\\n Test \\n\")\n",
    "for i in range(len(ssa_test_data)):\n",
    "    ssa_test_0.append(SSA(ssa_test_data[i,0,:,0], window))\n",
    "    ssa_test_1.append(SSA(ssa_test_data[i,1,:,0], window))\n",
    "    if(i%100==1):\n",
    "        sys.stdout.write(\"\\rNow: \"+str(np.round(i*100/len(ssa_test_data), 2))+\"%\")\n",
    "        sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With 1 components:\n",
      "___________________________________________________\n",
      "Model Size = 158040\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.35098, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.35098 to 0.45912, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.45912 to 0.57642, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.57642 to 0.61747, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.61747 to 0.65385, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.65385 to 0.71528, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.71528 to 0.76698, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.76698 to 0.77706, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.77706 to 0.79537, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.79537 to 0.81477, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00011: val_acc improved from 0.81477 to 0.82776, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00012: val_acc improved from 0.82776 to 0.85123, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.85123\n",
      "\n",
      "Epoch 00014: val_acc improved from 0.85123 to 0.85539, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00015: val_acc improved from 0.85539 to 0.87296, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.87296\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.87296\n",
      "\n",
      "Epoch 00018: val_acc improved from 0.87296 to 0.89111, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.89111\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.89111\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.89111\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.89111\n",
      "\n",
      "Epoch 00023: val_acc improved from 0.89111 to 0.89877, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.89877\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.89877\n",
      "\n",
      "Epoch 00026: val_acc improved from 0.89877 to 0.90027, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00027: val_acc improved from 0.90027 to 0.90618, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.90618\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.90618\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.90618\n",
      "\n",
      "Epoch 00031: val_acc improved from 0.90618 to 0.91067, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.91067\n",
      "***[RESULT]*** ID Accuracy: 83.67999999999999\n",
      "***[RESULT]*** ID  Confusion Matrix\n",
      "[71.7 88.9 95.6 88.6 83.7 76.7 86.2 84.9 80.4 82.1 61.8 77.5 90.7 57.\n",
      " 78.8 95.7 71.  71.6 89.6 84.6 98.9 94.6 83.4 92. ]\n",
      "***[RESULT]*** ID Averaged F-1 Score : 0.8279729812962838\n",
      "With 2 components:\n",
      "___________________________________________________\n",
      "Model Size = 158040\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.32784, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.32784 to 0.51765, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.51765 to 0.62179, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.62179 to 0.67166, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.67166 to 0.71270, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.71270 to 0.80569, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.80569 to 0.83433, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.83433 to 0.85823, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.85823 to 0.86921, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.86921 to 0.88853, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00011: val_acc improved from 0.88853 to 0.90118, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.90118\n",
      "\n",
      "Epoch 00013: val_acc improved from 0.90118 to 0.91250, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.91250\n",
      "\n",
      "Epoch 00015: val_acc improved from 0.91250 to 0.92033, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.92033\n",
      "\n",
      "Epoch 00017: val_acc improved from 0.92033 to 0.92408, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00018: val_acc improved from 0.92408 to 0.93032, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.93032\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.93032\n",
      "\n",
      "Epoch 00021: val_acc improved from 0.93032 to 0.93831, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00022: val_acc improved from 0.93831 to 0.94289, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.94289\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.94289\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.94289\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.94289\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.94289\n",
      "\n",
      "Epoch 00028: val_acc improved from 0.94289 to 0.94347, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.94347\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.94347\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.94347\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.94347\n",
      "***[RESULT]*** ID Accuracy: 88.47\n",
      "***[RESULT]*** ID  Confusion Matrix\n",
      "[83.2 98.3 98.  85.5 90.9 82.5 89.7 97.4 81.4 83.7 73.  90.7 94.4 73.1\n",
      " 80.4 97.9 82.3 57.1 88.6 89.1 99.3 96.8 95.4 94. ]\n",
      "***[RESULT]*** ID Averaged F-1 Score : 0.8788185662593486\n",
      "With 3 components:\n",
      "___________________________________________________\n",
      "Model Size = 158040\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.37646, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.37646 to 0.54679, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.54679 to 0.62637, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.62637 to 0.70663, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.70663 to 0.82867, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.82867 to 0.83733, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.83733 to 0.86330, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.86330 to 0.87870, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.87870\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.87870 to 0.91126, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00011: val_acc improved from 0.91126 to 0.92582, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00012: val_acc improved from 0.92582 to 0.93007, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.93007\n",
      "\n",
      "Epoch 00014: val_acc improved from 0.93007 to 0.93049, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00015: val_acc improved from 0.93049 to 0.93906, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00016: val_acc improved from 0.93906 to 0.94689, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.94689\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.94689\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.94689\n",
      "\n",
      "Epoch 00020: val_acc improved from 0.94689 to 0.94847, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.94847\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.94847\n",
      "\n",
      "Epoch 00023: val_acc improved from 0.94847 to 0.95463, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.95463\n",
      "\n",
      "Epoch 00025: val_acc improved from 0.95463 to 0.95604, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.95604\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.95604\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.95604\n",
      "\n",
      "Epoch 00029: val_acc improved from 0.95604 to 0.95946, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.95946\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.95946\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.95946\n",
      "***[RESULT]*** ID Accuracy: 90.83\n",
      "***[RESULT]*** ID  Confusion Matrix\n",
      "[86.9 99.3 98.  91.2 86.5 81.3 88.7 93.2 87.  95.  78.2 87.9 93.5 53.8\n",
      " 90.6 94.5 86.3 91.3 98.9 93.8 97.  96.4 95.7 92. ]\n",
      "***[RESULT]*** ID Averaged F-1 Score : 0.9011224131632366\n",
      "With 4 components:\n",
      "___________________________________________________\n",
      "Model Size = 158040\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.38328, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.38328 to 0.51390, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.51390 to 0.65210, saving model to XXID.best.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00004: val_acc improved from 0.65210 to 0.75982, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.75982 to 0.83908, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.83908 to 0.88195, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.88195 to 0.89777, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.89777 to 0.91434, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.91434 to 0.92799, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.92799 to 0.93124, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00011: val_acc improved from 0.93124 to 0.93606, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00012: val_acc improved from 0.93606 to 0.94514, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00013: val_acc improved from 0.94514 to 0.95147, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.95147\n",
      "\n",
      "Epoch 00015: val_acc improved from 0.95147 to 0.95230, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.95230\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.95230\n",
      "\n",
      "Epoch 00018: val_acc improved from 0.95230 to 0.95246, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00019: val_acc improved from 0.95246 to 0.95496, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00020: val_acc improved from 0.95496 to 0.95521, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00021: val_acc improved from 0.95521 to 0.95804, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00022: val_acc improved from 0.95804 to 0.96528, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.96528\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.96528\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.96528\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.96528\n",
      "\n",
      "Epoch 00027: val_acc improved from 0.96528 to 0.97053, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.97053\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.97053\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.97053\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.97053\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.97053\n",
      "***[RESULT]*** ID Accuracy: 91.36999999999999\n",
      "***[RESULT]*** ID  Confusion Matrix\n",
      "[80.7 99.8 97.3 83.8 87.4 90.6 90.1 95.4 97.4 95.4 75.6 90.  92.9 58.1\n",
      " 88.6 98.9 94.2 75.2 99.4 95.7 97.2 96.6 96.6 94. ]\n",
      "***[RESULT]*** ID Averaged F-1 Score : 0.9060514528886765\n",
      "With 5 components:\n",
      "___________________________________________________\n",
      "Model Size = 158040\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.30195, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.30195 to 0.57326, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.57326 to 0.57684, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.57684 to 0.80186, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.80186 to 0.87446, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.87446 to 0.89852, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.89852 to 0.90951, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.90951 to 0.92707, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.92707 to 0.93482, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.93482\n",
      "\n",
      "Epoch 00011: val_acc improved from 0.93482 to 0.93964, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00012: val_acc improved from 0.93964 to 0.95504, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.95504\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.95504\n",
      "\n",
      "Epoch 00015: val_acc improved from 0.95504 to 0.96129, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.96129\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.96129\n",
      "\n",
      "Epoch 00018: val_acc improved from 0.96129 to 0.96753, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.96753\n",
      "\n",
      "Epoch 00020: val_acc improved from 0.96753 to 0.97211, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.97211\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.97211\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.97211\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.97211\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.97211\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.97211\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.97211\n",
      "***[RESULT]*** ID Accuracy: 92.27\n",
      "***[RESULT]*** ID  Confusion Matrix\n",
      "[76.3 97.9 99.1 90.4 92.6 88.6 92.4 94.8 96.2 96.8 78.9 93.  96.9 53.4\n",
      " 87.6 99.7 90.5 87.9 99.5 95.8 99.2 98.2 96.5 89.4]\n",
      "***[RESULT]*** ID Averaged F-1 Score : 0.9131232100931586\n",
      "With 6 components:\n",
      "___________________________________________________\n",
      "Model Size = 158040\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.34807, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.34807 to 0.55012, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.55012 to 0.71737, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.71737 to 0.83267, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.83267 to 0.85856, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.85856 to 0.89211, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.89211 to 0.92832, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.92832 to 0.93282, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.93282 to 0.94589, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.94589 to 0.94922, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.94922\n",
      "\n",
      "Epoch 00012: val_acc improved from 0.94922 to 0.94963, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00013: val_acc improved from 0.94963 to 0.96254, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.96254\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.96254\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.96254\n",
      "\n",
      "Epoch 00017: val_acc improved from 0.96254 to 0.96495, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.96495\n",
      "\n",
      "Epoch 00019: val_acc improved from 0.96495 to 0.97003, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00020: val_acc improved from 0.97003 to 0.97145, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00021: val_acc improved from 0.97145 to 0.97294, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.97294\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.97294\n",
      "\n",
      "Epoch 00024: val_acc improved from 0.97294 to 0.97502, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.97502\n",
      "\n",
      "Epoch 00026: val_acc improved from 0.97502 to 0.97511, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00027: val_acc improved from 0.97511 to 0.97694, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.97694\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.97694\n",
      "\n",
      "Epoch 00030: val_acc improved from 0.97694 to 0.97786, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00031: val_acc improved from 0.97786 to 0.97794, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.97794\n",
      "***[RESULT]*** ID Accuracy: 94.48\n",
      "***[RESULT]*** ID  Confusion Matrix\n",
      "[ 84.6 100.   98.2  90.1  93.   88.2  96.1  98.8  94.8  99.5  73.5  94.5\n",
      "  99.7  76.4  92.7  96.9  92.9  95.3  99.5  98.4  99.7  99.2  93.8  95.7]\n",
      "***[RESULT]*** ID Averaged F-1 Score : 0.9383943135784875\n",
      "With 7 components:\n",
      "___________________________________________________\n",
      "Model Size = 158040\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.33758, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.33758 to 0.47070, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.47070 to 0.70013, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.70013 to 0.78605, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.78605 to 0.86455, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.86455 to 0.89344, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.89344 to 0.92499, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.92499 to 0.92807, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.92807 to 0.93307, saving model to XXID.best.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00010: val_acc improved from 0.93307 to 0.95413, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.95413\n",
      "\n",
      "Epoch 00012: val_acc improved from 0.95413 to 0.95621, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.95621\n",
      "\n",
      "Epoch 00014: val_acc improved from 0.95621 to 0.96429, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.96429\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.96429\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.96429\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.96429\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.96429\n",
      "\n",
      "Epoch 00020: val_acc improved from 0.96429 to 0.96637, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00021: val_acc improved from 0.96637 to 0.97028, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.97028\n",
      "\n",
      "Epoch 00023: val_acc improved from 0.97028 to 0.97469, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00024: val_acc improved from 0.97469 to 0.97527, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.97527\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.97527\n",
      "\n",
      "Epoch 00027: val_acc improved from 0.97527 to 0.97586, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.97586\n",
      "\n",
      "Epoch 00029: val_acc improved from 0.97586 to 0.97719, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00030: val_acc improved from 0.97719 to 0.97985, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.97985\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.97985\n",
      "***[RESULT]*** ID Accuracy: 93.88\n",
      "***[RESULT]*** ID  Confusion Matrix\n",
      "[ 93.1 100.   99.5  91.5  92.1  83.1  94.3  93.7  94.6  99.1  85.8  86.3\n",
      "  99.7  71.4  94.5  99.4  90.3  95.8  99.   92.2  99.2  95.8  93.7  97.4]\n",
      "***[RESULT]*** ID Averaged F-1 Score : 0.9345153899396502\n",
      "With 8 components:\n",
      "___________________________________________________\n",
      "Model Size = 158040\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.33483, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.33483 to 0.51632, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.51632 to 0.65426, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.65426 to 0.71412, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.71412 to 0.74592, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.74592 to 0.88545, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.88545 to 0.90593, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.90593 to 0.90709, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.90709 to 0.93440, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.93440 to 0.94505, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00011: val_acc improved from 0.94505 to 0.96087, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.96087\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.96087\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.96087\n",
      "\n",
      "Epoch 00015: val_acc improved from 0.96087 to 0.96578, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00016: val_acc improved from 0.96578 to 0.97011, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.97011\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.97011\n",
      "\n",
      "Epoch 00019: val_acc improved from 0.97011 to 0.97294, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.97294\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.97294\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.97294\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.97294\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.97294\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.97294\n",
      "\n",
      "Epoch 00026: val_acc improved from 0.97294 to 0.97577, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.97577\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.97577\n",
      "\n",
      "Epoch 00029: val_acc improved from 0.97577 to 0.97677, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.97677\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.97677\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.97677\n",
      "***[RESULT]*** ID Accuracy: 91.75\n",
      "***[RESULT]*** ID  Confusion Matrix\n",
      "[76.3 99.5 98.  90.9 96.3 82.5 95.4 98.6 94.  95.  86.3 94.7 93.5 56.\n",
      " 92.7 96.6 92.7 71.4 98.7 90.3 99.9 98.  93.5 88.6]\n",
      "***[RESULT]*** ID Averaged F-1 Score : 0.9103742050946199\n",
      "With 9 components:\n",
      "___________________________________________________\n",
      "Model Size = 158040\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.30644, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.30644 to 0.52514, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.52514 to 0.66001, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.66001 to 0.75699, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.75699 to 0.82093, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.82093 to 0.87005, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.87005 to 0.91225, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.91225 to 0.92341, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.92341 to 0.93973, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.93973\n",
      "\n",
      "Epoch 00011: val_acc improved from 0.93973 to 0.95480, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00012: val_acc improved from 0.95480 to 0.95588, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00013: val_acc improved from 0.95588 to 0.95688, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00014: val_acc improved from 0.95688 to 0.96678, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.96678\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.96678\n",
      "\n",
      "Epoch 00017: val_acc improved from 0.96678 to 0.96778, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00018: val_acc improved from 0.96778 to 0.97228, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00019: val_acc improved from 0.97228 to 0.97353, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.97353\n",
      "\n",
      "Epoch 00021: val_acc improved from 0.97353 to 0.97478, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.97478\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.97478\n",
      "\n",
      "Epoch 00024: val_acc improved from 0.97478 to 0.97711, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.97711\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.97711\n",
      "\n",
      "Epoch 00027: val_acc improved from 0.97711 to 0.97935, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.97935\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.97935\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.97935\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.97935\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.97935\n",
      "***[RESULT]*** ID Accuracy: 92.07\n",
      "***[RESULT]*** ID  Confusion Matrix\n",
      "[81.4 99.8 96.8 86.  91.2 85.1 94.7 96.  93.4 98.8 77.3 93.  97.5 47.5\n",
      " 95.7 99.4 96.6 84.3 99.7 95.1 99.7 98.8 88.3 92.9]\n",
      "***[RESULT]*** ID Averaged F-1 Score : 0.9112390417002011\n",
      "With 10 components:\n",
      "___________________________________________________\n",
      "Model Size = 158040\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.37529, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.37529 to 0.55037, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.55037 to 0.65368, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.65368 to 0.80919, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.80919 to 0.83933, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.83933 to 0.89269, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.89269 to 0.90842, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.90842 to 0.92025, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.92025 to 0.94214, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.94214\n",
      "\n",
      "Epoch 00011: val_acc improved from 0.94214 to 0.94930, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.94930\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.94930\n",
      "\n",
      "Epoch 00014: val_acc improved from 0.94930 to 0.96603, saving model to XXID.best.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00015: val_acc did not improve from 0.96603\n",
      "\n",
      "Epoch 00016: val_acc improved from 0.96603 to 0.96845, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.96845\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.96845\n",
      "\n",
      "Epoch 00019: val_acc improved from 0.96845 to 0.96945, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.96945\n",
      "\n",
      "Epoch 00021: val_acc improved from 0.96945 to 0.97261, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00022: val_acc improved from 0.97261 to 0.97577, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.97577\n",
      "\n",
      "Epoch 00024: val_acc improved from 0.97577 to 0.97611, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.97611\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.97611\n",
      "\n",
      "Epoch 00027: val_acc improved from 0.97611 to 0.97702, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.97702\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.97702\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.97702\n",
      "\n",
      "Epoch 00031: val_acc improved from 0.97702 to 0.97827, saving model to XXID.best.hdf5\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.97827\n",
      "***[RESULT]*** ID Accuracy: 92.54\n",
      "***[RESULT]*** ID  Confusion Matrix\n",
      "[88.  99.5 98.6 94.2 96.  81.7 93.6 92.7 96.2 99.1 70.4 91.8 98.8 37.7\n",
      " 93.7 98.2 95.  94.2 99.  94.6 99.7 96.6 97.1 89.7]\n",
      "***[RESULT]*** ID Averaged F-1 Score : 0.9153658253091729\n"
     ]
    }
   ],
   "source": [
    "id_history = {}\n",
    "ep = 32\n",
    "for num_comps in range(1,11):\n",
    "    ssa_train_data = train_data.copy()\n",
    "    ssa_test_data = test_data.copy()\n",
    "    \n",
    "    print(\"With \"+str(num_comps)+\" components:\")\n",
    "    for i in range(len(ssa_train_data)):\n",
    "        ssa_train_data[i,0,:,0] = ssa_train_0[i].reconstruct(list(range(0,num_comps)))\n",
    "        ssa_train_data[i,1,:,0] = ssa_train_1[i].reconstruct(list(range(0,num_comps)))\n",
    "    for i in range(len(ssa_test_data)):\n",
    "        ssa_test_data[i,0,:,0] = ssa_test_0[i].reconstruct(list(range(0,num_comps)))\n",
    "        ssa_test_data[i,1,:,0] = ssa_test_1[i].reconstruct(list(range(0,num_comps)))\n",
    "    \n",
    "    height = train_data.shape[1]\n",
    "    width = train_data.shape[2]\n",
    "\n",
    "    id_class_numbers = 24\n",
    "    act_class_numbers = 4\n",
    "    fm = (2,5)\n",
    "\n",
    "    print(\"___________________________________________________\")\n",
    "    ## Callbacks\n",
    "    eval_metric= \"val_acc\"    \n",
    "    #eval_metric= \"val_f1_metric\"    \n",
    "    early_stop = keras.callbacks.EarlyStopping(monitor=eval_metric, mode='max', patience = 7)\n",
    "    filepath=\"XXID.best.hdf5\"\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor=eval_metric, verbose=1, save_best_only=True, mode='max')\n",
    "    callbacks_list = [checkpoint, early_stop]\n",
    "    ## Callbacks\n",
    "\n",
    "    eval_act = Estimator.build(height, width, id_class_numbers, name =\"EVAL_ACT\", fm=fm, act_func=\"softmax\",hid_act_func=\"relu\")\n",
    "    eval_act.compile( loss=\"categorical_crossentropy\", optimizer='adam', metrics=['acc',f1_metric])\n",
    "    print(\"Model Size = \"+str(eval_act.count_params()))\n",
    "\n",
    "    eval_act.fit(ssa_train_data, id_train_labels,\n",
    "                validation_split = .2,\n",
    "                epochs = ep,\n",
    "                batch_size = 128,\n",
    "                verbose = 0,\n",
    "                class_weight = get_class_weights(np.argmax(id_train_labels,axis=1)),\n",
    "                callbacks = callbacks_list\n",
    "               )\n",
    "\n",
    "    eval_act.load_weights(\"XXID.best.hdf5\")\n",
    "    eval_act.compile( loss=\"categorical_crossentropy\", optimizer='adam', metrics=['acc',f1_metric])\n",
    "\n",
    "    result1 = eval_act.evaluate(ssa_test_data, id_test_labels, verbose = 2)\n",
    "    act_acc = result1[1].round(4)*100\n",
    "    print(\"***[RESULT]*** ID Accuracy: \"+str(act_acc))\n",
    "\n",
    "    preds = eval_act.predict(ssa_test_data)\n",
    "    preds = np.argmax(preds, axis=1)\n",
    "    conf_mat = confusion_matrix(np.argmax(id_test_labels, axis=1), preds)\n",
    "    conf_mat = conf_mat.astype('float') / conf_mat.sum(axis=1)[:, np.newaxis]\n",
    "    print(\"***[RESULT]*** ID  Confusion Matrix\")\n",
    "    print((np.array(conf_mat).diagonal()).round(3)*100)  \n",
    "\n",
    "    f1act = f1_score(np.argmax(id_test_labels, axis=1), preds, average=None).mean()\n",
    "    print(\"***[RESULT]*** ID Averaged F-1 Score : \"+str(f1act))\n",
    "\n",
    "    id_history[num_comps] = f1act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 0.8279729812962838,\n",
       " 2: 0.8788185662593486,\n",
       " 3: 0.9011224131632366,\n",
       " 4: 0.9060514528886765,\n",
       " 5: 0.9131232100931586,\n",
       " 6: 0.9383943135784875,\n",
       " 7: 0.9345153899396502,\n",
       " 8: 0.9103742050946199,\n",
       " 9: 0.9112390417002011,\n",
       " 10: 0.9153658253091729}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
